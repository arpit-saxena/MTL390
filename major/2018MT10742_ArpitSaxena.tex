\documentclass[12pt, oneside]{article}
\usepackage{a4wide}
\usepackage{oldgerm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{cancel}
\setlength{\textheight}{8.875in} \setlength{\textwidth}{6.875in}
\setlength{\columnsep}{0.3125in} \setlength{\topmargin}{0in}
\setlength{\headheight}{0in} \setlength{\headsep}{0in}
\setlength{\parindent}{1pc} \setlength{\oddsidemargin}{-.304in}
\setlength{\evensidemargin}{-.304in}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

\begin{document}
\setlength{\textheight}{8.5in}
\centering {\bf MTL 390 (Statistical Methods) }\\


\centering{\bf Major Examination Assignment 2 Report}



\vskip 0.5cm

\noindent Name: Arpit Saxena ~~~  ~~~~~ ~~~~ ~~~~~~~~~~~~~~~~ Entry Number: 2018MT10742 ~~~~~~~~~~~



\vskip 0.5cm



\begin{enumerate}
	



\item	Testing of Hypothesis


\item	 Testing of Hypothesis


\item	Analysis of correlation and regression 



\item	Analysis of correlation and regression 


\item {
    Time Series Analysis

    Show that every stationary MA(2) process without a unit root can be converted to an invertible
    process by suitably changing the coefficients and white noise random variables.
    Note that this can actually be done for all MA(q) processes. Is the following
    MA(2) process invertible?
    \[X_t = e_t - 3.2 e_{t-1} + 0.6 e_{t-2}\]
    Here \(e_t\)'s are white noise random variables normally distributed with mean
    0 and variance 2. If not, convert it to an invertible MA(2) process.

    \textbf{Answer}

    A general MA(2) process looks like the following:
    \begin{align*}
        X_t &= e_t + \lambda_1 e_{t-1} + \lambda_2 e_{t-2} \\
        \intertext{We can use the backshift operator \(B\) to write}
        X_t &= e_t + \lambda_1 B(e_{t}) + \lambda_2 B^2(e_{t}) \\
        \implies X_t &= (1 + \lambda_1 B + \lambda_2 B^2) e_{t} \\
        \intertext{We factorise the polynomial into linear factors to get}
        X_t &= (1 - \mu_1 B)(1 - \mu_2 B) e_t
    \end{align*}

    Thus we get the general characteristic polynomial of the MA(2) process as
    \((1 - \mu_1 B)(1 - \mu_2 B)\). The zeros of the polynomial are \(\frac{1}{\mu_1}\)
    and \(\frac{1}{\mu_2}\) respectively.

    We note that the way to find if the process is invertible is that the absolute
    value of the zeros of the characteristic polynomial are greater than 1. If not, there would be atleast one zero which is less than 1. We note that the
    absence of unit roots means that no zero is exactly equal to 1.

    We show that a process with the characteristic polynomial with one root \(\mu\) taken
    as the reciprocal and variance of the white noise multiplied by \(\mu^2\) yields
    a process with the same autocovariance function.

    Let us define \(\lambda_0 = 1\). Then,
    \begin{align*}
        Cov(X_t, X_{t+\tau}) &= E(X_t X_{t+\tau}) - E(X_t)E(X_{t+\tau}) \\
            &= E(X_t X_{t+\tau}) \tag*{(since \(E(X_t) = 0 \,\forall t\))} \\
            &= E\left[\left\{\sum_{j=0}^{q}\lambda_j e_{t-j}\right\}\left\{\sum_{k=0}^{q}\lambda_k e_{t-k+\tau}\right\}\right] \\
            &= \sum_{j=0}^{q} \sum_{k=0}^{q} \lambda_j \lambda_k E(e_{t-j} e_{t-k+\tau}) \\
        \intertext{Since \(e_t\)'s are independent and identically distributed with variance as
        \(\sigma^2\)}
        Cov(X_t, X_{t+\tau}) &= \sum_{j=0}^{q} \sum_{k=0}^{q} \lambda_j \lambda_k \sigma^2 \delta_{t-j,t-k+\tau}
            \tag*{(Where \(\delta_{ij}\) is the Kronecker delta)}
    \end{align*}

    To remove the Kronecker delta, We make the following observations:
    \begin{itemize}
        \item When \(\tau = 0\), the \((j, k)\) pairs which will be included are \((0, 0), (1, 1), \ldots, (q, q)\)
        \item For \(\tau = 1\), we'll have \((0, 1), (1, 2), \ldots, (q-1, q)\)
        \item For \(\tau = 2\), we'll have \((0, 2), (1, 3), \ldots, (q-2, q)\)
        \item \(\vdots\)
        \item For \(\tau = q-1\), we'll have \((0, q-1), (1, q)\)
        \item For \(\tau = q\), we'll have \((0, q)\)
        \item For \(\tau > q\), \(t - j \neq t - k + \tau \,\forall j, k = 0, \ldots, q\)
    \end{itemize}

    So we'll have the following simplification:
    \begin{equation}
        Cov(X_t, X_{t+\tau}) = \begin{cases}
            \sigma^2 \sum_{j=0}^{q-\tau} \lambda_j \lambda_{j+\tau} & \text{if } \tau = 0, 1, \ldots, q \\
            0 & \tau > q
        \end{cases}
    \end{equation}

    Now for a process \(X_t = (1 - \mu_1 B)(1 - \mu_2 B)e_t\), we have \(\lambda_1 = -\mu_1 - \mu_2\)
    and \(\lambda_2 = \mu_1 \mu_2\), then we get
    \begin{align*}
        Cov(X_t, X_t) &= \sigma^2 [1 + (\mu_1 + \mu_2)^2 + \mu_1^2 \mu_2^2] \\
        Cov(X_t, X_{t+1}) &= \sigma^2 [(-\mu_1-\mu_2) + (-\mu_1-\mu_2)(\mu_1 \mu_2)] \\
            &= - \sigma^2 (\mu_1 + \mu_2) (1 + \mu_1 \mu_2) \\
        Cov(X_t, X_{t+2}) &= \sigma^2 \mu_1 \mu_2 \\
        Cov(X_t, X_{t+\tau}) &= 0 \,\forall \tau > 2
    \end{align*}

    If we replace \(\mu_1\) by \(\frac{1}{\mu_1}\) in the above equations, we'll get
    \begin{align*}
        Cov(X_t, X_t) &= \sigma^2 \left[1 + \left(\frac{1}{\mu_1} + \mu_2\right)^2 + \frac{1}{\mu_1^2} \mu_2^2\right] \\
            &= \frac{\sigma^2}{\mu_1^2} \left[\mu_1^2 + \left(1 + \mu_1 \mu_2\right)^2 + \mu_2^2\right] \\
            &= \frac{\sigma^2}{\mu_1^2} \left[\mu_1^2 + 1 + 2 \mu_1 \mu_2 + \mu_1^2 \mu_2^2 + \mu_2^2\right] \\
            &= \frac{\sigma^2}{\mu_1^2} \left[1 + (\mu_1 + \mu_2)^2 + \mu_1^2 \mu_2^2\right] \\
        Cov(X_t, X_{t+1}) &= - \sigma^2 \left(\frac{1}{\mu_1} + \mu_2\right) \left(1 + \frac{1}{\mu_1} \mu_2\right) \\
            &= - \frac{\sigma^2}{\mu_1^2} \left(1 + \mu_1 \mu_2\right) \left(\mu_1 + \mu_2\right) \\
        Cov(X_t, X_{t+2}) &= \sigma^2 \frac{1}{\mu_1} \mu_2 \\
        &= \frac{\sigma^2}{\mu_1^2} ~\mu_1 \mu_2 \\
        Cov(X_t, X_{t+\tau}) &= 0 \,\forall \tau > 2
    \end{align*}

    We observe that we have the exact same autocovariance equations with the variance
    \(\sigma^2\) divided by \(\mu^2\).
    
    Thus, we can change a root of the characteristic polynomial to its reciprocal by suitably
    changing the variance of the white noise variables to get the same autocovariance function.
    This way, we can change all roots which are less than 1 to become more than 1 and obtain an
    invertible process this way.

    For the process \(X_t = e_t - 3.2 e_{t-1} + 0.6 e_{t-2}\), the characteristic polynomial
    is \(\theta(B) = 1 - 3.2 B + 0.6 B^2 = (1 - 3 B)(1 - 0.2 B)\). The zeros of this
    polynomial are \(\frac{1}{3}, 5\). Since one of them is less than 1, this process is not
    invertible.

    We can make this invertible by the procedure outlined above. We'll replace the root
    \(\frac{1}{3}\) by 3 and change the variance of white noise which is currently 2 to
    \(\frac{2}{\frac{1}{3}^2} = 18\). Thus, we obtain the process
    \begin{align*}
        X_t &= (1 - \frac{1}{3} B)(1 - 0.2 B) e_t \\
            &= e_t - \frac{8}{15} e_{t-1} + \frac{1}{15} e_{t-2}
    \end{align*}
    where \(e_t\)'s are white noise variables normally distributed with mean 0 and variance 18.
}


\item {
    Time Series Analysis

    Consider the stationary ARMA(p, q) process. Elaborate the general method of finding the
    variance and covariances. Use the method to find the variance and covariances \&
    correlations of time difference upto 3 (i.e. \(\rho_1, \rho_2, \rho_3\)) of the stationary ARMA(1, 1)
    process.

    \textbf{Answer}

    Let \(\left\{X_t\right\}_{t \in \mathbb{N}}\) be a time series following the
    stationary ARMA(p, q) process. Then we have the following recurrence relation:

    \[X_t = \sum_{j=1}^{p}\alpha_j X_{t-j} + \sum_{j=1}^{q} \beta_j e_{t-j} + e_t\]

    where \(e_t\)'s are independent and identically distributed random variables following
    a normal distribution with mean 0 and variance 1. We also take the boundary condition as
    \(X_t = 0 ~\forall t < max(p, q)\)

    Since this is a stationary process, it is implied that it will also satisfy
    wide sense stationarity, which means that
    \begin{align*}
        E(X_t) &= \mu ~\forall t \\
        Cov(X_t, X_s) &= f(t - s)
    \end{align*}

    i.e. the mean of all the random variables of the process is constant and
    the covariance of the random variables at two time instances of the process
    depends only on the time difference between them.

    We first find the mean of the random variables of the process:
    \begin{align*}
        X_t &= \sum_{j=1}^{p}\alpha_j X_{t-j} + \sum_{j=1}^{q} \beta_j e_{t-j} + e_t \\
        \implies E(X_t) &= E\left(\sum_{j=1}^{p}\alpha_j X_{t-j} + \sum_{j=1}^{q} \beta_j e_{t-j} + e_t\right) \\
        \implies E(X_t) &= \sum_{j=1}^{p}\alpha_j E(X_{t-j}) + \sum_{j=1}^{q} \beta_j E(e_{t-j}) + E(e_t)
        \intertext{Now we use the fact that \(e_t\)'s have a mean of 0 to get}
        E(X_t) &= \sum_{j=1}^{p}\alpha_j E(X_{t-j}) \\
        \implies \mu &= \sum_{j=1}^{p}\alpha_j \mu \tag*{(Since the process is stationary)} \\
        \implies \left(\sum_{j=1}^{p}\alpha_j - 1\right) \mu &= 0 \\
        \intertext{Now assuming that \(\sum_{j=1}^{p}\alpha_j \neq 0\), we get}
        \mu &= 0
    \end{align*}

    Therefore we have the following result:
    \begin{equation}
        E(X_t) = 0 ~\forall t \label{eq:q6:meanzero}
    \end{equation}

    We denote \(\gamma_\tau\) as the covariance of random variables in this process
    at time \(\tau\) apart. Then,
    \begin{align*}
        X_t &= \sum_{j=1}^{p}\alpha_j X_{t-j} + \sum_{j=1}^{q} \beta_j e_{t-j} + e_t \\
        \intertext{Multiplying by \(X_{t-\tau}\) on both sides, we get}
        X_t X_{t-\tau} &= \sum_{j=1}^{p}\alpha_j X_{t-j} X_{t-\tau} + \sum_{j=1}^{q} \beta_j e_{t-j} X_{t-\tau} + e_t X_{t-\tau} \\
        \implies E\left(X_t X_{t-\tau}\right) &= E\left(\sum_{j=1}^{p}\alpha_j X_{t-j} X_{t-\tau} + \sum_{j=1}^{q} \beta_j e_{t-j} X_{t-\tau} + e_t X_{t-\tau}\right) \\
        \implies E\left(X_t X_{t-\tau}\right) &= \sum_{j=1}^{p}\alpha_j E\left(X_{t-j} X_{t-\tau}\right) + \sum_{j=1}^{q} \beta_j E\left(e_{t-j} X_{t-\tau}\right) + E\left(e_t X_{t-\tau}\right) \tag*{(Since expectation is a linear operator)} \\
        \intertext{Using \eqref{eq:q6:meanzero}, we have \(Cov(X_t, X_s) = E(X_t X_s) - E(X_t)E(X_s) = E(X_t X_s)\)}
        \implies Cov\left(X_t, X_{t-\tau}\right) &= \sum_{j=1}^{p}\alpha_j Cov\left(X_{t-j}, X_{t-\tau}\right) + \sum_{j=1}^{q} \beta_j E\left(e_{t-j} X_{t-\tau}\right) + E\left(e_t X_{t-\tau}\right) \\
        \implies \gamma_{\tau} &= \sum_{j=1}^{p}\alpha_j \gamma_{\tau - j} + \sum_{j=1}^{q} \beta_j E\left(e_{t-j} X_{t-\tau}\right) + E\left(e_t X_{t-\tau}\right)
    \end{align*}

    Now we note that \(X_{t-\tau}\) is a function of the white noise variables \(e_1,\ldots,e_{t-\tau}\) and since
    they are independent from each other, \(e_t\) is independent from all of \(e_1,\ldots,e_{t-\tau}\) and thus \(E(e_t X_{t-\tau}) = Cov(e_t, X_{t-\tau}) + \cancelto{0}{E(e_t)} E(X_{t-\tau}) = Cov(e_t, X_{t-\tau}) = 0\).
    Therefore, we get
    \begin{equation}
        \gamma_{\tau} = \sum_{j=1}^{p}\alpha_j \gamma_{\tau - j} + \sum_{j=1}^{q} \beta_j E\left(e_{t-j} X_{t-\tau}\right) \label{eq:q6:arma_var}
    \end{equation}

    Using the previous logic, we can see that \(E(e_t X_s) = 0\) whenever \(t > s\) i.e. \(e_t\) and \(X_s\) are
    independent whenever \(t > s\). In lieu of this observation, we split into the following two cases:

    \renewcommand{\labelitemi}{\textendash}
    \begin{itemize}
        \item \textbf{Case 1: } \(\tau > q\)
        
        Here, \(j < \tau \implies t - j > t - \tau\) for all \(j = 1, \ldots, q\). This implies, by our previous observation,
        that \(E(e_{t-j} X_{t-\tau}) = 0\) for all \(j = 1,\ldots,q\)

        Thus the equation simplifies to:
        \begin{equation}
            \gamma_{\tau} = \sum_{j=1}^{p}\alpha_j \gamma_{\tau - j} \label{eq:q6:case1}
        \end{equation}

        \item \textbf{Case 2: } \(0 < \tau \leq q\)
        
        In this case, we split up the summation into two parts as:
        \begin{align*}
            \sum_{j=1}^{q} \beta_j E(e_{t-j} X_{t-\tau}) &= \cancelto{0}{\sum_{j=1}^{\tau - 1} \beta_j E(e_{t-j} X_{t-\tau})} + \sum_{j=\tau}^{q} \beta_j E(e_{t-j} X_{t-\tau}) \\
                &= \sum_{j=\tau}^{q} \beta_j E(e_{t-j} X_{t-\tau})
        \end{align*}

        For each of the terms we'll need to expand \(X_{t-\tau}\) to find the coefficient
        of \(e_{t-j}\) in it, which will give us the expectation.

        Thus we get the equation as:
        \begin{equation}
            \gamma_{\tau} = \sum_{j=1}^{p}\alpha_j \gamma_{\tau - j} + \sum_{j=\tau}^{q} \beta_j E(e_{t-j} X_{t-\tau})
                \label{eq:q6:case2}
        \end{equation}
    \end{itemize}

    Now, we find the variance of \(X_t\).
    \begin{align*}
        X_t &= \sum_{j=1}^{p}\alpha_j X_{t-j} + \sum_{j=1}^{q} \beta_j e_{t-j} + e_t \\
        \intertext{Multiplying by \(X_t\) on both sides, we get}
        X_t^2 &= \sum_{j=1}^{p}\alpha_j X_{t-j} X_t + \sum_{j=1}^{q} \beta_j e_{t-j} X_t + e_t X_t \\
        \implies E(X_t^2) &= \sum_{j=1}^{p}\alpha_j E(X_{t-j} X_t) + \sum_{j=1}^{q} \beta_j E(e_{t-j} X_t) + E(e_t X_t) \\
        \implies Var(X_t) &= \sum_{j=1}^{p}\alpha_j \gamma_j + \sum_{j=1}^{q} \beta_j E(e_{t-j} X_t) + E\left(e_t \left\{\sum_{j=1}^{p}\alpha_j X_{t-j} + \sum_{j=1}^{q} \beta_j e_{t-j} + e_t\right\}\right) \\
        \implies Var(X_t) &= \sum_{j=1}^{p}\alpha_j \gamma_j + \sum_{j=1}^{q} \beta_j E(e_{t-j} X_t) + \sum_{j=1}^{p}\alpha_j \cancelto{0}{E(e_t X_{t-j})} + \sum_{j=1}^{q} \beta_j \cancelto{0}{E(e_t e_{t-j})} + E(e_t^2)
    \end{align*}
    
    Therefore, we have the variance equation as:
    \begin{equation}
        Var(X_t) = \sum_{j=1}^{p}\alpha_j \gamma_j + \sum_{j=1}^{q} \beta_j E(e_{t-j} X_t) + 1 \label{eq:q6:var}
    \end{equation}

    Now we consider a stationary ARMA(1, 1) process with the equation
    \[X_t = \alpha X_{t-1} + \beta e_{t-1} + e_t\]

    To find the variance, we use \eqref{eq:q6:var}.
    \begin{align*}
        Var(X_t) &= \sum_{j=1}^{p}\alpha_j \gamma_j + \sum_{j=1}^{q} \beta_j E(e_{t-j} X_t) + 1 \\
            &= \alpha \gamma_1 + \beta E(e_{t-1} X_t) + 1 \\
            &= \alpha \gamma_1 + \beta E\left(e_{t-1} \left\{\alpha X_{t-1} + \beta e_{t-1} + e_t\right\} \right) + 1 \\
            &= \alpha \gamma_1 + \alpha \beta E(e_{t-1} X_{t-1}) + \beta^2 E(e_{t-1}^2) + \beta \cancelto{0}{E(e_{t-1} e_t)} + 1
    \end{align*}

    Now using \(E(e_{t-1}^2 = Var(e_{t-1}) = 1)\) and \(E(e_t X_t) = 1 ~\forall t\), we get
    \begin{equation}
        Var(X_t) = \alpha \gamma_1 + \alpha \beta + \beta^2 + 1 \label{eq:q6:var_calc}
    \end{equation}

    Note we have yet to find \(\gamma_1\) which we'll do next. Note that since \(0 < \tau = 1 \leq q = 1\),
    we'll use \eqref{eq:q6:case2} to calculate.
    \begin{align*}
        \gamma_{\tau} &= \sum_{j=1}^{p}\alpha_j \gamma_{\tau - j} + \sum_{j=\tau}^{q} \beta_j E(e_{t-j} X_{t-\tau}) \\
        \implies \gamma_1 &= \alpha \gamma_0 + \beta E(e_{t-1} X_{t-1}) \\
        \implies \gamma_1 &= \alpha \gamma_0 + \beta \\
        \implies \gamma_1 &= \alpha Var(X_t) + \beta \numberthis \label{eq:q6:gamma1_var} \\
        \intertext{Now using \eqref{eq:q6:var_calc}, we get}
        \gamma_1 &= \alpha (\alpha \gamma_1 + \alpha \beta + \beta^2 + 1) + \beta \\
        \implies \gamma_1 &= \alpha^2 \gamma_1 + \alpha^2 \beta + \alpha \beta^2 + \alpha + \beta \\
        \implies (1 - \alpha^2) \gamma_1 &= \alpha^2 \beta + \alpha \beta^2 + \alpha + \beta \\
        \implies \gamma_1 &= \frac{\alpha^2 \beta + \alpha \beta^2 + \alpha + \beta}{1 - \alpha^2} 
            \numberthis \label{eq:q6:gamma1_ans}
    \end{align*}

    Now that we have found \(\gamma_1\), we'll describe the other results using it since
    they get very messy otherwise. From \eqref{eq:q6:gamma1_var}, we have
    \begin{align*}
        \gamma_1 &= \alpha Var(X_t) + \beta \\
        \implies \gamma_1 &= \alpha \gamma_0 + \beta \\
        \intertext{Now dividing both sides by \(\gamma_0\) and setting \(\frac{\gamma_1}{\gamma_0}\) to \(\rho_1\), we get}
        \rho_1 &= \alpha + \frac{\beta}{\gamma_0} \\
        \implies \rho_1 &= \alpha + \frac{\beta}{ \alpha \gamma_1 + \alpha \beta + \beta^2 + 1} \tag*{(Using \eqref{eq:q6:var_calc})}
    \end{align*}

    For \(\tau > 1 = q\), we can use the equation for case 1 i.e. \eqref{eq:q6:case1}. We have
    \begin{align*}
        \gamma_{\tau} &= \sum_{j=1}^{p}\alpha_j \gamma_{\tau - j} \\
        \implies \gamma_\tau &= \alpha \gamma_{\tau-1} \\
        \intertext{Now dividing both sides by the variance to the correlations,}
        \rho_\tau &= \alpha \rho_{\tau-1} \\
        \therefore~ \rho_2 &= \alpha \rho_1 = \alpha^2 + \frac{\alpha \beta}{ \alpha \gamma_1 + \alpha \beta + \beta^2 + 1} \\
        \rho_3 &= \alpha \rho_2 = \alpha^3 + \frac{\alpha^2 \beta}{ \alpha \gamma_1 + \alpha \beta + \beta^2 + 1}
    \end{align*}

    Therefore, we have found the values of \(\rho_1, \rho_2, \rho_3\) as
    \begin{align*}
        \rho_1 &= \alpha + \frac{\beta}{ \alpha \gamma_1 + \alpha \beta + \beta^2 + 1} \\
        \rho_2 &= \alpha^2 + \frac{\alpha \beta}{ \alpha \gamma_1 + \alpha \beta + \beta^2 + 1} \\
        \rho_3 &= \alpha^3 + \frac{\alpha^2 \beta}{ \alpha \gamma_1 + \alpha \beta + \beta^2 + 1}
        \intertext{where, }
        \gamma_1 &= \frac{\alpha^2 \beta + \alpha \beta^2 + \alpha + \beta}{1 - \alpha^2}
    \end{align*}
}	


\end{enumerate}

\end{document}
