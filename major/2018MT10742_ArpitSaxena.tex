\documentclass[12pt, oneside]{article}
\usepackage{a4wide}
\usepackage{oldgerm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{cancel}
\usepackage{mathtools}
\setlength{\textheight}{8.875in} \setlength{\textwidth}{6.875in}
\setlength{\columnsep}{0.3125in} \setlength{\topmargin}{0in}
\setlength{\headheight}{0in} \setlength{\headsep}{0in}
\setlength{\parindent}{1pc} \setlength{\oddsidemargin}{-.304in}
\setlength{\evensidemargin}{-.304in}
\renewcommand{\labelitemi}{\textendash}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

\begin{document}
\setlength{\textheight}{8.5in}
\centering {\bf MTL 390 (Statistical Methods) }\\


\centering{\bf Major Examination Assignment 2 Report}



\vskip 0.5cm

\noindent Name: Arpit Saxena ~~~  ~~~~~ ~~~~ ~~~~~~~~~~~~~~~~ Entry Number: 2018MT10742 ~~~~~~~~~~~



\vskip 0.5cm



\begin{enumerate}
	



\item	Testing of Hypothesis


\item	 Testing of Hypothesis


\item {
    Analysis of correlation and regression 

    Give the general formula for Spearman's rank correlation coefficient. Using it,
    derive the simpler formula when distinct ranks are assumed. Emphasise where the
    distinctness is assumed.

    Consider the following data. Find the Spearman's rank correlation coefficient
    using the general formula and the other formula with the distinct rank assumption.
    What is the difference in the results?

    \begin{center}
    \begin{tabular}{ | l | l | }
        \hline
        \textbf{X} & \textbf{Y} \\
        \hline
        106 &	7 \\
        100 &  	27 \\
        86 & 2 \\
        101 &	50 \\
        99 &	29 \\
        103 &	29 \\
        99 &	20 \\
        113 &	12 \\
        112 &	6 \\
        110 &	17 \\
        \hline
    \end{tabular}
    \end{center}

    \textbf{Answer}

    Let us consider a sample of size \(n\). The \(n\) raw scores \(X_i, Y_i\) are
    converted to ranks \(rg_{X_i}, rg_{Y_i}\), and \(r_s\) is computed as
    \begin{equation}
        r_s = \rho_{rg_X, rg_Y} = \frac{cov(rg_X, rg_Y)}{\sigma_{rg_x}\sigma_{rg_Y}}
            \label{eq:q3:spearman_general_formula}
    \end{equation}
    where,
    \begin{itemize}
        \item \(\rho\) denotes the Pearson correlation coefficient, but applied to 
            rank variables,
        \item \(cov(rg_X, rg_Y)\) is the covariance of the rank variables
        \item \(\rho_{rg_X}\) and \(\rho_{rg_Y}\) are the standard deviations of the rank variables
    \end{itemize}

    Let us directly work with ranks to derive the formula. We denote \(X_i\) and 
    \(Y_i\) as the ranks. Then for each of these

    \begin{align*}
        \sum_{i=1}^{n} X_i &= 1 + 2 + \cdots + n \\
            &= \frac{n(n+1)}{2} \\
        \intertext{We note that the assumption of distinct ranks doesn't come into 
        play here. This is because in case of ties an average of the tied ranks is
        given to both the variables and the sum becomes the same in that case as well}
        \overline{X} &= \frac{1}{n} \sum_{i=1}^{n} X_i \\
            &= \frac{n + 1}{2} \\
        \text{Similarly, } \overline{Y} &= \frac{n + 1}{2}
    \end{align*}

    Now we calculate the variance.
    \begin{align*}
        \sigma_X^2 &= \frac{1}{n} \sum_{i=1}^{n} (X_i - \overline{X})^2 \\
            &= \frac{1}{n} \sum_{i=1}^{n} (X_i^2 - 2 X_i \overline{X} + \overline{X}^2) \\
            &= \frac{1}{n} \left[ \sum_{i=1}^{n} X_i^2 - 2 \overline{X} \sum_{i=1}^n X_i + n \overline{{X}^2} \right] \\
            &= \frac{1}{n} \left[ \sum_{i=1}^{n} X_i^2 - 2 \overline{X} (n \overline{X}) + n \overline{{X}^2} \right] \\
            &= \frac{1}{n} \left[ \sum_{i=1}^{n} X_i^2 - n \overline{X}^2 \right] \\
        \intertext{We use the assumption of distinct ranks here to get}
        \sum_{i=1}^{n} X_i^2 &= 1^2 + 2^2 + \cdots + n^2 = \frac{n(n+1)(2n+1)}{6}
        \intertext{Plugging in this value and the mean calculated earlier, we get}
        \sigma_X^2 &= \frac{1}{n} \left[ \frac{n(n+1)(2n+1)}{6} - n \left\{\frac{n+1}{2}\right\}^2 \right] \\
            &= \frac{1}{n} \left[ \frac{n(n+1)(2n+1)}{6} - \frac{n(n + 1)^2}{4} \right] \\
            &= \frac{1}{n} \frac{n(n+1)}{12} \left[ 2(2n + 1) - 3(n+1) \right] \\
            &= \frac{n + 1}{12} (n - 1) \\
            &= \frac{n^2 - 1}{12} \\
        \text{Similarly, we get } \sigma_Y^2 &= \frac{n^2 - 1}{12}
    \end{align*}

    Having calculated the variance of \(X_i, Y_i\), we calculate their covariance.
    \begin{align*}
        Cov(X, Y) &= \frac{1}{n} \sum_{i=1}^{n} (X_i - \overline{X}) (Y_i - \overline{Y}) \\
            &= \frac{1}{n} \sum_{i=1}^{n} [X_i Y_i - X_i \overline{Y} - \overline{X} Y_i + \overline{X} \overline{Y}] \\
            &= \frac{1}{n} \left\{ \sum_{i=1}^{n} X_i Y_i - \overline{Y} \sum_{i=1}^{n} X_i - \overline{X} \sum_{i=1}^{n} Y_i + n \overline{X} \overline{Y} \right\} \\
            &= \frac{1}{n} \left\{ \sum_{i=1}^{n} X_i Y_i - n \overline{Y} \overline{X} - n \overline{X} \overline{Y} + n \overline{X} \overline{Y} \right\} \\
            &= \frac{1}{n} \left\{ \sum_{i=1}^{n} X_i Y_i - n \overline{X} \overline{Y}\right\} \\
            &= \frac{1}{n} \sum_{i=1}^{n} X_i Y_i - \overline{X} \overline{Y} \\
            &= -\frac{1}{2n} \sum_{i=1}^{n} \left\{ X_i^2 + Y_i^2 -2 X_i Y_i - (X_i^2 + Y_i^2) \right\} - \overline{X} \overline{Y} \\
            &= \frac{1}{2n} \sum_{i=1}^{n} \left\{ X_i^2 + Y_i^2 \right\} -\frac{1}{2n} \sum_{i=1}^{n} \left\{ X_i^2 + Y_i^2 -2 X_i Y_i \right\} - \overline{X} \overline{Y} \\
            \intertext{We are once again invoking the assumption of distinct ranks to calculate the sum of \(X_i^2\) to get}
        Cov(X, Y) &= \frac{1}{2n} \times 2\cdot \frac{n(n+1)(2n+1)}{6} - \frac{1}{2n} \sum_{i=1}^{n} \left\{ X_i - Y_i \right\}^2 - \overline{X} \overline{Y} \\
            &= \frac{(n + 1)(2n + 1)}{6} - \frac{1}{2n} \sum_{i=1}^{n} d_i^2 - \left\{ \frac{n+1}{2} \right\}^2 \\
            &= \frac{n + 1}{12} \{2(2n + 1) - 3(n + 1)\} - \frac{1}{2n} \sum_{i=1}^{n} d_i^2 \\
            &= \frac{n + 1}{12} \cdot (n - 1) - \frac{1}{2n} \sum_{i=1}^{n} d_i^2 \\
            &= \frac{n^2 - 1}{12} - \frac{1}{2n} \sum_{i=1}^{n} d_i^2
    \end{align*}

    Where \(d_i = X_i - Y_i\) is the difference of ranks.

    Now we calculate the Spearman rank correlation coefficient using \eqref{eq:q3:spearman_general_formula}
    \begin{align*}
        r_s &= \frac{Cov(X, Y)}{\sigma_X \sigma_Y} \\
            &= \frac{\frac{n^2 - 1}{12} - \frac{1}{2n} \sum_{i=1}^{n} d_i^2}{ \frac{n^2 - 1}{12} } \\
        \therefore~ \Aboxed{r_s  &= 1 - \frac{6}{n(n^2 - 1)} \sum_{i=1}^{n} d_i^2}
            \numberthis \label{eq:q3:spearman_distinct_formula}
    \end{align*}

    Thus we have obtained the formula for calculating the Spearman rank correlation coefficient in case
    of distinct ranks.

    Now for the given data, we first convert the raw scores into ranks. These are tabulated below:

    \begin{center}
        \begin{tabular}{ | c | c | c | c | c | c |}
            \hline
            X & Y & rank \(X_i\) & rank \(Y_i\) & \(d_i\) & \(d_i^2\)\\
            \hline
            106 &	7  & 7  &   3  & 4 & 16\\
            100 &  	27 & 4  &   7  & -3 & 9\\
            86 & 2     & 1  &   1  & 0 & 0\\
            101 &	50 & 5  &   10 & -5 & 25\\
            99 &	29 & 2.5  & 8.5  & -6 & 36\\
            103 &	29 & 6  &  8.5 & -2.5 & 6.25\\
            99 &	20 & 2.5  & 6  & -3.5 & 12.25\\
            113 &	12 & 10  &  4  & 6 & 36\\
            112 &	6  & 9  &   2 & 7 & 49\\
            110 &	17 & 8  &   5 & 3 & 9\\
            \hline
        \end{tabular}
    \end{center}

    Note that due to ties, there are some fractional ranks. Suppose there was a tie
    between values at ranks 2 and 3. Then we assign both of these rank 2.5 which is
    the mean of 2 and 3.

    Now, we calculate the spearman rank correlation coefficient using the general 
    formula \eqref{eq:q3:spearman_general_formula}.
    \begin{align*}
        Var(\text{rank } X) &= 8.2 \\
        Var(\text{rank } Y) &= 8.2 \\
        Cov(\text{rank } X, \text{rank } Y) &= -1.725 \\
        \therefore r_s &= \frac{Cov(\text{rank } X, \text{rank } Y)}{\sqrt{Var(\text{rank } X) ~Var(\text{rank } Y)}} \\
            &= \frac{-1.725}{\sqrt{8.2 \times 8.2}} \\
            &= -0.210
    \end{align*}

    Now, using the Spearman rank correlation coefficient distinct rank formula
    \eqref{eq:q3:spearman_distinct_formula}.
    \begin{align*}
        r_s &= 1 - \frac{6}{n(n^2 - 1)} \sum_{i=1}^{n} d_i^2 \\
            &= -0.203
    \end{align*}

    We note that there is only an absolute difference of 0.007 in these calculated
    values which is about 3.5\% relative difference. We note that it's a small
    difference and the distinct rank formula may be useful even in case of ties
    for approximation purposes.
}

\item	Analysis of correlation and regression 


\item {
    Time Series Analysis

    Show that every stationary MA(2) process without a unit root can be converted to an invertible
    process by suitably changing the coefficients and white noise random variables.
    Note that this can actually be done for all MA(q) processes. Is the following
    MA(2) process invertible?
    \[X_t = e_t - 3.2 e_{t-1} + 0.6 e_{t-2}\]
    Here \(e_t\)'s are white noise random variables normally distributed with mean
    0 and variance 2. If not, convert it to an invertible MA(2) process.

    \textbf{Answer}

    A general MA(2) process looks like the following:
    \begin{align*}
        X_t &= e_t + \lambda_1 e_{t-1} + \lambda_2 e_{t-2} \\
        \intertext{We can use the backshift operator \(B\) to write}
        X_t &= e_t + \lambda_1 B(e_{t}) + \lambda_2 B^2(e_{t}) \\
        \implies X_t &= (1 + \lambda_1 B + \lambda_2 B^2) e_{t} \\
        \intertext{We factorise the polynomial into linear factors to get}
        X_t &= (1 - \mu_1 B)(1 - \mu_2 B) e_t
    \end{align*}

    Thus we get the general characteristic polynomial of the MA(2) process as
    \((1 - \mu_1 B)(1 - \mu_2 B)\). The zeros of the polynomial are \(\frac{1}{\mu_1}\)
    and \(\frac{1}{\mu_2}\) respectively.

    We note that the way to find if the process is invertible is that the absolute
    value of the zeros of the characteristic polynomial are greater than 1. If not, there would be atleast one zero which is less than 1. We note that the
    absence of unit roots means that no zero is exactly equal to 1.

    We show that a process with the characteristic polynomial with one root \(\mu\) taken
    as the reciprocal and variance of the white noise multiplied by \(\mu^2\) yields
    a process with the same autocovariance function.

    Let us define \(\lambda_0 = 1\). Then,
    \begin{align*}
        Cov(X_t, X_{t+\tau}) &= E(X_t X_{t+\tau}) - E(X_t)E(X_{t+\tau}) \\
            &= E(X_t X_{t+\tau}) \tag*{(since \(E(X_t) = 0 \,\forall t\))} \\
            &= E\left[\left\{\sum_{j=0}^{q}\lambda_j e_{t-j}\right\}\left\{\sum_{k=0}^{q}\lambda_k e_{t-k+\tau}\right\}\right] \\
            &= \sum_{j=0}^{q} \sum_{k=0}^{q} \lambda_j \lambda_k E(e_{t-j} e_{t-k+\tau}) \\
        \intertext{Since \(e_t\)'s are independent and identically distributed with variance as
        \(\sigma^2\)}
        Cov(X_t, X_{t+\tau}) &= \sum_{j=0}^{q} \sum_{k=0}^{q} \lambda_j \lambda_k \sigma^2 \delta_{t-j,t-k+\tau}
            \tag*{(Where \(\delta_{ij}\) is the Kronecker delta)}
    \end{align*}

    To remove the Kronecker delta, We make the following observations:
    \begin{itemize}
        \item When \(\tau = 0\), the \((j, k)\) pairs which will be included are \((0, 0), (1, 1), \ldots, (q, q)\)
        \item For \(\tau = 1\), we'll have \((0, 1), (1, 2), \ldots, (q-1, q)\)
        \item For \(\tau = 2\), we'll have \((0, 2), (1, 3), \ldots, (q-2, q)\)
        \item \(\vdots\)
        \item For \(\tau = q-1\), we'll have \((0, q-1), (1, q)\)
        \item For \(\tau = q\), we'll have \((0, q)\)
        \item For \(\tau > q\), \(t - j \neq t - k + \tau \,\forall j, k = 0, \ldots, q\)
    \end{itemize}

    So we'll have the following simplification:
    \begin{equation}
        Cov(X_t, X_{t+\tau}) = \begin{cases}
            \sigma^2 \sum_{j=0}^{q-\tau} \lambda_j \lambda_{j+\tau} & \text{if } \tau = 0, 1, \ldots, q \\
            0 & \tau > q
        \end{cases}
    \end{equation}

    Now for a process \(X_t = (1 - \mu_1 B)(1 - \mu_2 B)e_t\), we have \(\lambda_1 = -\mu_1 - \mu_2\)
    and \(\lambda_2 = \mu_1 \mu_2\), then we get
    \begin{align*}
        Cov(X_t, X_t) &= \sigma^2 [1 + (\mu_1 + \mu_2)^2 + \mu_1^2 \mu_2^2] \\
        Cov(X_t, X_{t+1}) &= \sigma^2 [(-\mu_1-\mu_2) + (-\mu_1-\mu_2)(\mu_1 \mu_2)] \\
            &= - \sigma^2 (\mu_1 + \mu_2) (1 + \mu_1 \mu_2) \\
        Cov(X_t, X_{t+2}) &= \sigma^2 \mu_1 \mu_2 \\
        Cov(X_t, X_{t+\tau}) &= 0 \,\forall \tau > 2
    \end{align*}

    If we replace \(\mu_1\) by \(\frac{1}{\mu_1}\) in the above equations, we'll get
    \begin{align*}
        Cov(X_t, X_t) &= \sigma^2 \left[1 + \left(\frac{1}{\mu_1} + \mu_2\right)^2 + \frac{1}{\mu_1^2} \mu_2^2\right] \\
            &= \frac{\sigma^2}{\mu_1^2} \left[\mu_1^2 + \left(1 + \mu_1 \mu_2\right)^2 + \mu_2^2\right] \\
            &= \frac{\sigma^2}{\mu_1^2} \left[\mu_1^2 + 1 + 2 \mu_1 \mu_2 + \mu_1^2 \mu_2^2 + \mu_2^2\right] \\
            &= \frac{\sigma^2}{\mu_1^2} \left[1 + (\mu_1 + \mu_2)^2 + \mu_1^2 \mu_2^2\right] \\
        Cov(X_t, X_{t+1}) &= - \sigma^2 \left(\frac{1}{\mu_1} + \mu_2\right) \left(1 + \frac{1}{\mu_1} \mu_2\right) \\
            &= - \frac{\sigma^2}{\mu_1^2} \left(1 + \mu_1 \mu_2\right) \left(\mu_1 + \mu_2\right) \\
        Cov(X_t, X_{t+2}) &= \sigma^2 \frac{1}{\mu_1} \mu_2 \\
        &= \frac{\sigma^2}{\mu_1^2} ~\mu_1 \mu_2 \\
        Cov(X_t, X_{t+\tau}) &= 0 \,\forall \tau > 2
    \end{align*}

    We observe that we have the exact same autocovariance equations with the variance
    \(\sigma^2\) divided by \(\mu^2\).
    
    Thus, we can change a root of the characteristic polynomial to its reciprocal by suitably
    changing the variance of the white noise variables to get the same autocovariance function.
    This way, we can change all roots which are less than 1 to become more than 1 and obtain an
    invertible process this way.

    For the process \(X_t = e_t - 3.2 e_{t-1} + 0.6 e_{t-2}\), the characteristic polynomial
    is \(\theta(B) = 1 - 3.2 B + 0.6 B^2 = (1 - 3 B)(1 - 0.2 B)\). The zeros of this
    polynomial are \(\frac{1}{3}, 5\). Since one of them is less than 1, this process is not
    invertible.

    We can make this invertible by the procedure outlined above. We'll replace the root
    \(\frac{1}{3}\) by 3 and change the variance of white noise which is currently 2 to
    \(\frac{2}{\frac{1}{3}^2} = 18\). Thus, we obtain the process
    \begin{align*}
        X_t &= (1 - \frac{1}{3} B)(1 - 0.2 B) e_t \\
            &= e_t - \frac{8}{15} e_{t-1} + \frac{1}{15} e_{t-2}
    \end{align*}
    where \(e_t\)'s are white noise variables normally distributed with mean 0 and variance 18.
}


\item {
    Time Series Analysis

    Consider the stationary ARMA(p, q) process. Elaborate the general method of finding the
    variance and covariances. Use the method to find the variance and covariances \&
    correlations of time difference upto 3 (i.e. \(\rho_1, \rho_2, \rho_3\)) of the stationary ARMA(1, 1)
    process.

    \textbf{Answer}

    Let \(\left\{X_t\right\}_{t \in \mathbb{N}}\) be a time series following the
    stationary ARMA(p, q) process. Then we have the following recurrence relation:

    \[X_t = \sum_{j=1}^{p}\alpha_j X_{t-j} + \sum_{j=1}^{q} \beta_j e_{t-j} + e_t\]

    where \(e_t\)'s are independent and identically distributed random variables following
    a normal distribution with mean 0 and variance 1. We also take the boundary condition as
    \(X_t = 0 ~\forall t < max(p, q)\)

    Since this is a stationary process, it is implied that it will also satisfy
    wide sense stationarity, which means that
    \begin{align*}
        E(X_t) &= \mu ~\forall t \\
        Cov(X_t, X_s) &= f(t - s)
    \end{align*}

    i.e. the mean of all the random variables of the process is constant and
    the covariance of the random variables at two time instances of the process
    depends only on the time difference between them.

    We first find the mean of the random variables of the process:
    \begin{align*}
        X_t &= \sum_{j=1}^{p}\alpha_j X_{t-j} + \sum_{j=1}^{q} \beta_j e_{t-j} + e_t \\
        \implies E(X_t) &= E\left(\sum_{j=1}^{p}\alpha_j X_{t-j} + \sum_{j=1}^{q} \beta_j e_{t-j} + e_t\right) \\
        \implies E(X_t) &= \sum_{j=1}^{p}\alpha_j E(X_{t-j}) + \sum_{j=1}^{q} \beta_j E(e_{t-j}) + E(e_t)
        \intertext{Now we use the fact that \(e_t\)'s have a mean of 0 to get}
        E(X_t) &= \sum_{j=1}^{p}\alpha_j E(X_{t-j}) \\
        \implies \mu &= \sum_{j=1}^{p}\alpha_j \mu \tag*{(Since the process is stationary)} \\
        \implies \left(\sum_{j=1}^{p}\alpha_j - 1\right) \mu &= 0 \\
        \intertext{Now assuming that \(\sum_{j=1}^{p}\alpha_j \neq 0\), we get}
        \mu &= 0
    \end{align*}

    Therefore we have the following result:
    \begin{equation}
        E(X_t) = 0 ~\forall t \label{eq:q6:meanzero}
    \end{equation}

    We denote \(\gamma_\tau\) as the covariance of random variables in this process
    at time \(\tau\) apart. Then,
    \begin{align*}
        X_t &= \sum_{j=1}^{p}\alpha_j X_{t-j} + \sum_{j=1}^{q} \beta_j e_{t-j} + e_t \\
        \intertext{Multiplying by \(X_{t-\tau}\) on both sides, we get}
        X_t X_{t-\tau} &= \sum_{j=1}^{p}\alpha_j X_{t-j} X_{t-\tau} + \sum_{j=1}^{q} \beta_j e_{t-j} X_{t-\tau} + e_t X_{t-\tau} \\
        \implies E\left(X_t X_{t-\tau}\right) &= E\left(\sum_{j=1}^{p}\alpha_j X_{t-j} X_{t-\tau} + \sum_{j=1}^{q} \beta_j e_{t-j} X_{t-\tau} + e_t X_{t-\tau}\right) \\
        \implies E\left(X_t X_{t-\tau}\right) &= \sum_{j=1}^{p}\alpha_j E\left(X_{t-j} X_{t-\tau}\right) + \sum_{j=1}^{q} \beta_j E\left(e_{t-j} X_{t-\tau}\right) + E\left(e_t X_{t-\tau}\right) \tag*{(Since expectation is a linear operator)} \\
        \intertext{Using \eqref{eq:q6:meanzero}, we have \(Cov(X_t, X_s) = E(X_t X_s) - E(X_t)E(X_s) = E(X_t X_s)\)}
        \implies Cov\left(X_t, X_{t-\tau}\right) &= \sum_{j=1}^{p}\alpha_j Cov\left(X_{t-j}, X_{t-\tau}\right) + \sum_{j=1}^{q} \beta_j E\left(e_{t-j} X_{t-\tau}\right) + E\left(e_t X_{t-\tau}\right) \\
        \implies \gamma_{\tau} &= \sum_{j=1}^{p}\alpha_j \gamma_{\tau - j} + \sum_{j=1}^{q} \beta_j E\left(e_{t-j} X_{t-\tau}\right) + E\left(e_t X_{t-\tau}\right)
    \end{align*}

    Now we note that \(X_{t-\tau}\) is a function of the white noise variables \(e_1,\ldots,e_{t-\tau}\) and since
    they are independent from each other, \(e_t\) is independent from all of \(e_1,\ldots,e_{t-\tau}\) and thus \(E(e_t X_{t-\tau}) = Cov(e_t, X_{t-\tau}) + \cancelto{0}{E(e_t)} E(X_{t-\tau}) = Cov(e_t, X_{t-\tau}) = 0\).
    Therefore, we get
    \begin{equation}
        \gamma_{\tau} = \sum_{j=1}^{p}\alpha_j \gamma_{\tau - j} + \sum_{j=1}^{q} \beta_j E\left(e_{t-j} X_{t-\tau}\right) \label{eq:q6:arma_var}
    \end{equation}

    Using the previous logic, we can see that \(E(e_t X_s) = 0\) whenever \(t > s\) i.e. \(e_t\) and \(X_s\) are
    independent whenever \(t > s\). In lieu of this observation, we split into the following two cases:

    \renewcommand{\labelitemi}{\textendash}
    \begin{itemize}
        \item \textbf{Case 1: } \(\tau > q\)
        
        Here, \(j < \tau \implies t - j > t - \tau\) for all \(j = 1, \ldots, q\). This implies, by our previous observation,
        that \(E(e_{t-j} X_{t-\tau}) = 0\) for all \(j = 1,\ldots,q\)

        Thus the equation simplifies to:
        \begin{equation}
            \gamma_{\tau} = \sum_{j=1}^{p}\alpha_j \gamma_{\tau - j} \label{eq:q6:case1}
        \end{equation}

        \item \textbf{Case 2: } \(0 < \tau \leq q\)
        
        In this case, we split up the summation into two parts as:
        \begin{align*}
            \sum_{j=1}^{q} \beta_j E(e_{t-j} X_{t-\tau}) &= \cancelto{0}{\sum_{j=1}^{\tau - 1} \beta_j E(e_{t-j} X_{t-\tau})} + \sum_{j=\tau}^{q} \beta_j E(e_{t-j} X_{t-\tau}) \\
                &= \sum_{j=\tau}^{q} \beta_j E(e_{t-j} X_{t-\tau})
        \end{align*}

        For each of the terms we'll need to expand \(X_{t-\tau}\) to find the coefficient
        of \(e_{t-j}\) in it, which will give us the expectation.

        Thus we get the equation as:
        \begin{equation}
            \gamma_{\tau} = \sum_{j=1}^{p}\alpha_j \gamma_{\tau - j} + \sum_{j=\tau}^{q} \beta_j E(e_{t-j} X_{t-\tau})
                \label{eq:q6:case2}
        \end{equation}
    \end{itemize}

    Now, we find the variance of \(X_t\).
    \begin{align*}
        X_t &= \sum_{j=1}^{p}\alpha_j X_{t-j} + \sum_{j=1}^{q} \beta_j e_{t-j} + e_t \\
        \intertext{Multiplying by \(X_t\) on both sides, we get}
        X_t^2 &= \sum_{j=1}^{p}\alpha_j X_{t-j} X_t + \sum_{j=1}^{q} \beta_j e_{t-j} X_t + e_t X_t \\
        \implies E(X_t^2) &= \sum_{j=1}^{p}\alpha_j E(X_{t-j} X_t) + \sum_{j=1}^{q} \beta_j E(e_{t-j} X_t) + E(e_t X_t) \\
        \implies Var(X_t) &= \sum_{j=1}^{p}\alpha_j \gamma_j + \sum_{j=1}^{q} \beta_j E(e_{t-j} X_t) + E\left(e_t \left\{\sum_{j=1}^{p}\alpha_j X_{t-j} + \sum_{j=1}^{q} \beta_j e_{t-j} + e_t\right\}\right) \\
        \implies Var(X_t) &= \sum_{j=1}^{p}\alpha_j \gamma_j + \sum_{j=1}^{q} \beta_j E(e_{t-j} X_t) + \sum_{j=1}^{p}\alpha_j \cancelto{0}{E(e_t X_{t-j})} + \sum_{j=1}^{q} \beta_j \cancelto{0}{E(e_t e_{t-j})} + E(e_t^2)
    \end{align*}
    
    Therefore, we have the variance equation as:
    \begin{equation}
        Var(X_t) = \sum_{j=1}^{p}\alpha_j \gamma_j + \sum_{j=1}^{q} \beta_j E(e_{t-j} X_t) + 1 \label{eq:q6:var}
    \end{equation}

    Now we consider a stationary ARMA(1, 1) process with the equation
    \[X_t = \alpha X_{t-1} + \beta e_{t-1} + e_t\]

    To find the variance, we use \eqref{eq:q6:var}.
    \begin{align*}
        Var(X_t) &= \sum_{j=1}^{p}\alpha_j \gamma_j + \sum_{j=1}^{q} \beta_j E(e_{t-j} X_t) + 1 \\
            &= \alpha \gamma_1 + \beta E(e_{t-1} X_t) + 1 \\
            &= \alpha \gamma_1 + \beta E\left(e_{t-1} \left\{\alpha X_{t-1} + \beta e_{t-1} + e_t\right\} \right) + 1 \\
            &= \alpha \gamma_1 + \alpha \beta E(e_{t-1} X_{t-1}) + \beta^2 E(e_{t-1}^2) + \beta \cancelto{0}{E(e_{t-1} e_t)} + 1
    \end{align*}

    Now using \(E(e_{t-1}^2 = Var(e_{t-1}) = 1)\) and \(E(e_t X_t) = 1 ~\forall t\), we get
    \begin{equation}
        Var(X_t) = \alpha \gamma_1 + \alpha \beta + \beta^2 + 1 \label{eq:q6:var_calc}
    \end{equation}

    Note we have yet to find \(\gamma_1\) which we'll do next. Note that since \(0 < \tau = 1 \leq q = 1\),
    we'll use \eqref{eq:q6:case2} to calculate.
    \begin{align*}
        \gamma_{\tau} &= \sum_{j=1}^{p}\alpha_j \gamma_{\tau - j} + \sum_{j=\tau}^{q} \beta_j E(e_{t-j} X_{t-\tau}) \\
        \implies \gamma_1 &= \alpha \gamma_0 + \beta E(e_{t-1} X_{t-1}) \\
        \implies \gamma_1 &= \alpha \gamma_0 + \beta \\
        \implies \gamma_1 &= \alpha Var(X_t) + \beta \numberthis \label{eq:q6:gamma1_var} \\
        \intertext{Now using \eqref{eq:q6:var_calc}, we get}
        \gamma_1 &= \alpha (\alpha \gamma_1 + \alpha \beta + \beta^2 + 1) + \beta \\
        \implies \gamma_1 &= \alpha^2 \gamma_1 + \alpha^2 \beta + \alpha \beta^2 + \alpha + \beta \\
        \implies (1 - \alpha^2) \gamma_1 &= \alpha^2 \beta + \alpha \beta^2 + \alpha + \beta \\
        \implies \gamma_1 &= \frac{\alpha^2 \beta + \alpha \beta^2 + \alpha + \beta}{1 - \alpha^2} 
            \numberthis \label{eq:q6:gamma1_ans}
    \end{align*}

    Now that we have found \(\gamma_1\), we'll describe the other results using it since
    they get very messy otherwise. From \eqref{eq:q6:gamma1_var}, we have
    \begin{align*}
        \gamma_1 &= \alpha Var(X_t) + \beta \\
        \implies \gamma_1 &= \alpha \gamma_0 + \beta \\
        \intertext{Now dividing both sides by \(\gamma_0\) and setting \(\frac{\gamma_1}{\gamma_0}\) to \(\rho_1\), we get}
        \rho_1 &= \alpha + \frac{\beta}{\gamma_0} \\
        \implies \rho_1 &= \alpha + \frac{\beta}{ \alpha \gamma_1 + \alpha \beta + \beta^2 + 1} \tag*{(Using \eqref{eq:q6:var_calc})}
    \end{align*}

    For \(\tau > 1 = q\), we can use the equation for case 1 i.e. \eqref{eq:q6:case1}. We have
    \begin{align*}
        \gamma_{\tau} &= \sum_{j=1}^{p}\alpha_j \gamma_{\tau - j} \\
        \implies \gamma_\tau &= \alpha \gamma_{\tau-1} \\
        \intertext{Now dividing both sides by the variance to the correlations,}
        \rho_\tau &= \alpha \rho_{\tau-1} \\
        \therefore~ \rho_2 &= \alpha \rho_1 = \alpha^2 + \frac{\alpha \beta}{ \alpha \gamma_1 + \alpha \beta + \beta^2 + 1} \\
        \rho_3 &= \alpha \rho_2 = \alpha^3 + \frac{\alpha^2 \beta}{ \alpha \gamma_1 + \alpha \beta + \beta^2 + 1}
    \end{align*}

    Therefore, we have found the values of \(\rho_1, \rho_2, \rho_3\) as
    \begin{align*}
        \rho_1 &= \alpha + \frac{\beta}{ \alpha \gamma_1 + \alpha \beta + \beta^2 + 1} \\
        \rho_2 &= \alpha^2 + \frac{\alpha \beta}{ \alpha \gamma_1 + \alpha \beta + \beta^2 + 1} \\
        \rho_3 &= \alpha^3 + \frac{\alpha^2 \beta}{ \alpha \gamma_1 + \alpha \beta + \beta^2 + 1}
        \intertext{where, }
        \gamma_1 &= \frac{\alpha^2 \beta + \alpha \beta^2 + \alpha + \beta}{1 - \alpha^2}
    \end{align*}
}	


\end{enumerate}

\end{document}
