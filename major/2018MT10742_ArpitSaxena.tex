\documentclass[12pt, oneside]{article}
\usepackage{a4wide}
\usepackage{oldgerm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{cancel}
\usepackage{mathtools}
\usepackage{tabularx}
\usepackage{physics}
\setlength{\textheight}{8.875in} \setlength{\textwidth}{6.875in}
\setlength{\columnsep}{0.3125in} \setlength{\topmargin}{0in}
\setlength{\headheight}{0in} \setlength{\headsep}{0in}
\setlength{\parindent}{1pc} \setlength{\oddsidemargin}{-.304in}
\setlength{\evensidemargin}{-.304in}
\renewcommand{\labelitemi}{\textendash}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

\begin{document}
\setlength{\textheight}{8.5in}
\centering {\bf MTL 390 (Statistical Methods) }\\


\centering{\bf Major Examination Assignment 2 Report}



\vskip 0.5cm

\noindent Name: Arpit Saxena ~~~  ~~~~~ ~~~~ ~~~~~~~~~~~~~~~~ Entry Number: 2018MT10742 ~~~~~~~~~~~



\vskip 0.5cm



\begin{enumerate}
	



\item {
    Testing of Hypothesis

    Let \((X_1, X_2, \ldots, X_m)\) be a random sample from a Binomial Distribution
    \(B(n, p)\) with \(p\) unknown. Consider the hypothesis test of the null hypothesis
    \(H_0: p = p_0\) where \(p_0 \in [0, 1]\) is fixed, against the alternative
    composite hypothesis \(H_1: p > p_0\). Find the uniformly most powerful test of size
    \(\alpha\).

    Following are the number of heads obtained on tossing a coin 5 times and repeating
    it for 5 times. At 95\% level of confidence, test the null hypothesis of the coin
    to be a fair coin against the alternative hypothesis of the coin to be heads-biased 
    (i.e. it lands on heads with more probability than it does on tails).

    Number of heads: 3 4 3 5 3

    \textbf{Answer}

    The joint pdf of \((X_1, X_2, \ldots, X_m)\) is
    \begin{align*}
        L(p; x_1, x_2, \ldots, x_n) &= \prod_{i = 1}^{m} \left\{ {n \choose x_i} p^{x_i} (1 - p)^{n - x_i} \right\} \\
            &= \prod_{i=1}^{m} {n \choose x_i} ~ p^{\sum x_i} (1 - p)^{mn - \sum x_i}
    \end{align*}

    Let \(p'\) be a number greater than \(p_0\), then we take ratio of joint probability
    with parameter \(p_0\) to that of \(p'\) and let \(C\) be the set of points such that
    the ratio is less than a constant k.
    \begin{align*}
        \left(\frac{p}{p'}\right)^{\sum x_i} \left(\frac{1-p}{1-p'}\right)^{mn - \sum x_i} &\leq k
        \intertext{Now taking \(\log\) on both sides, we get}
        \log\left\{\frac{p}{p'}\right\}\cdot\sum_{i=1}^{m} x_i + \log\left\{\frac{1 - p}{1 - p'}\right\}\cdot\left(mn - \sum_{i=1}^{m} x_i\right) &\leq \log{k} \\
        \implies \sum_{i=1}^{m} x_i \cdot \left[ \log\left\{\frac{1 - p}{1 - p'}\right\} - \log\left\{\frac{p}{p'}\right\} \right] &\geq mn \log\left\{\frac{1 - p}{1 - p'}\right\} - \log{k} \\
        \implies \sum_{i=1}^{m} x_i &\geq \frac{mn \log\left\{\frac{1 - p}{1 - p'}\right\} - \log{k}}{\log\left\{\frac{1 - p}{1 - p'}\right\} - \log\left\{\frac{p}{p'}\right\}} = c
    \end{align*}

    Now we invoke Neyman-Pearson lemma to show that the region
    \[C = \left\{ (x_1, x_2, \ldots, x_2): \sum_{i=1}^{m} x_i \geq c \right\}\]
    is the best rejection region for testing the simple hypothesis \(H_0: p = p_0\)
    against the simple alternative hypothesis \(H_1: p = p'\).

    Now we need to find \(c\) so that the rejection region \(C\) is of the desired
    size \(\alpha\). Under the null hypothesis, the random variable \(\sum_{i=1}^m X_i\)
    follows the Binomial distribution \(B(mn, p_0)\) because sum of binomial variables
    with the same probability \(p\) is a binomial variable with the first terms added.

    Now we have to use the equation
    \[\alpha \leq P\left(\sum_{i=1}^{m} X_i \geq c\right)\] to find \(c\). Note that for a
    binomial distribution like we have here, \(c\) has no closed form expression.
    OncAFtere we find \(c\) using numerical methods, the region
    \[C = \left\{ (x_1, x_2, \ldots, x_2): \sum_{i=1}^{m} x_i \geq c \right\}\]
    is the best critical region of size \(\alpha\) for testing \(H_0: p = p_0\)
    against the alternative hypothesis \(H_1: p = p'\).

    Observe that this region will be valid for all \(p' > p_0\). Therefore, the
    region \(C\) is a uniformly most powerful critical region of size \(\alpha\)
    for testing \(H_0: p = p_0\) against \(H_1: p > p_0\).

    For the coin toss example, we have \(p_0 = 0.5\) and we solve
    \begin{align*}
        \alpha &\leq P\left(\sum_{i=1}^{m} X_i \geq c\right) \\
        \implies 0.05 &\leq (0.5)^{25} \left[ {25 \choose c} + {25 \choose {c + 1}} + \cdots + {25 \choose 25} \right]
    \end{align*}

    Since we have a small number at hand, we can just use brute-force to find the value of \(c\).

    We find that \(c \leq 17\) satisfies the inequality.

    From the given data, \(\sum x_i = 3 + 4 + 3 + 5 + 3 = 18 > 17\), and thus lies outside
    the critical region. Thus the null hypothesis of the coin to be a fair coin can
    be rejected at 95\% confidence level.
}


\item {
    Testing of Hypothesis

    Number of claims filed at 3 insurance companies (in millions) over 5 years are 
    shown in the following table.
    Using ANOVA method, test the appropriate hypothesis at 5\% level of significance
    to decide if the mean number of claims filed in companies 1, 2 and 3 differ.

    \begin{center}
    \begin{tabularx}{0.6\linewidth}{|X|X|X|}
        \hline
        Company 1 & Company 2 & Company 3 \\
        \hline
        15 & 13 & 23 \\
        17 & 21 & 16 \\
        16 & 15 & 14 \\
        14 & 16 & 14 \\
        13 & 18 & 18 \\
        \hline
    \end{tabularx}
    \end{center}

    \textbf{Answer}

    We consider the following null and alternate hypotheses.
    \begin{align*}
        H_0 &: \mu_1 = \mu_2 = \mu_3 \\
        H_1 &: \text{At least one pair of sample means is significantly different}
    \end{align*}

    Computing group means,
    \begin{align*}
        \overline{x}_1 &= \frac{15 + 17 + 16 + 14 + 13}{5} = 15 \\
        \overline{x}_2 &= \frac{13 + 21 + 15 + 16 + 18}{5} = 16.6 \\
        \overline{x}_3 &= \frac{23 + 16 + 14 + 14 + 18}{5} = 17
    \end{align*}

    Computing grand mean
    \[\overline{x}_G = \frac{\overline{x}_1 + \overline{x}_2 + \overline{x}_1}{3} = 16.2\]

    Now we compute \(\text{Sum of squares}_{\text{between}}\):
    \begin{align*}
        SS_b &= n \sum_{i=1}^{k} (\overline{x}_i - \overline{x}_G)^2 \\
            &= 5 \times [ (15 - 16.2)^2 + (16.6 - 16.2)^2 + (17 - 16.2)^2 ] \\
            &= 11.2
    \end{align*}

    Then we need to compute \(\text{Sum of squares}_{\text{within}}\)
    \begin{align*}
        SS_w &= \sum_{i=1}^{k} \sum_{j=1}^{n} (x_{ij} - \overline{x}_i)^2 \\
             &= [ (15 - 15)^2 + (17 - 15)^2 + (16 - 15)^2 + (14 - 15)^2 + (13-15)^2 \\
             &~~    + (13 - 16.6)^2 + (21 - 16.6)^2 + (15 - 16.6)^2 + (16 - 16.6)^2 + (18 - 16.6)^2 \\
             &~~    + (23 - 17)^2 + (16 - 17)^2 + (14 - 17)^2 + (14 - 17)^2 + (18 - 17)^2 ] \\
             &= 103.2
    \end{align*}
    
    Finally we have the \(\text{Sum of squares}_{\text{total}}\)
    \begin{align*}
        SS_{\text{total}} &= \sum_{i=1}^{k} \sum_{j=1}^{n} (x_{ij} - \overline{x}_G)^2 \\
            &= [ (15 - 16.2)^2 + (17 - 16.2)^2 + (16 - 16.2)^2 + (14 - 16.2)^2 + (13-16.2)^2 \\
            &~~    + (13 - 16.2)^2 + (21 - 16.2)^2 + (15 - 16.2)^2 + (16 - 16.2)^2 + (18 - 16.2)^2 \\
            &~~    + (23 - 16.2)^2 + (16 - 16.2)^2 + (14 - 16.2)^2 + (14 - 16.2)^2 + (18 - 16.2)^2 ] \\
            &= 114.4
    \end{align*}

    Degrees of freedom for \(SS_b\) are \(df_b = k - 1 = 2\).

    Degrees of freedom for \(SS_w\) are \(df_w = k(n - 1) = 3 \times 4 = 12\)

    We now compute the statistics
    \begin{align*}
        {\text{mean square}}_{\text{between}}: MS_{\text{between}} &= \frac{SS_b}{df_b} = \frac{11.2}{2} = 5.6 \\
        \text{mean square}_{\text{within}}: MS_{\text{within}} &= \frac{SS_w}{df_w} = \frac{103.2}{12} = 8.6 \\
        \text{F-statistic } F_0 &= \frac{MS_{\text{between}}}{MS_{\text{within}}} = \frac{5.6}{8.6} = 0.651
    \end{align*}

    From the statistics tables, we find that
    \[F_{k-1,N-k,\alpha} = F_{2,12,0.05} = 19.40\]

    We note that the calculated F-statistic is less than the critical value, so
    the null hypothesis is not rejected.

    Therefore, we can't reject the hypothesis that the number of claims submitted to
    the 3 insurance companies are same at 95\% level of significance.
}

\item {
    Analysis of correlation and regression 

    Give the general formula for Spearman's rank correlation coefficient. Using it,
    derive the simpler formula when distinct ranks are assumed. Emphasise where the
    distinctness is assumed.

    Consider the following data. Find the Spearman's rank correlation coefficient
    using the general formula and the other formula with the distinct rank assumption.
    What is the difference in the results?

    \begin{center}
    \begin{tabular}{ | l | l | }
        \hline
        \textbf{X} & \textbf{Y} \\
        \hline
        106 &	7 \\
        100 &  	27 \\
        86 & 2 \\
        101 &	50 \\
        99 &	29 \\
        103 &	29 \\
        99 &	20 \\
        113 &	12 \\
        112 &	6 \\
        110 &	17 \\
        \hline
    \end{tabular}
    \end{center}

    \textbf{Answer}

    Let us consider a sample of size \(n\). The \(n\) raw scores \(X_i, Y_i\) are
    converted to ranks \(rg_{X_i}, rg_{Y_i}\), and \(r_s\) is computed as
    \begin{equation}
        r_s = \rho_{rg_X, rg_Y} = \frac{cov(rg_X, rg_Y)}{\sigma_{rg_x}\sigma_{rg_Y}}
            \label{eq:q3:spearman_general_formula}
    \end{equation}
    where,
    \begin{itemize}
        \item \(\rho\) denotes the Pearson correlation coefficient, but applied to 
            rank variables,
        \item \(cov(rg_X, rg_Y)\) is the covariance of the rank variables
        \item \(\rho_{rg_X}\) and \(\rho_{rg_Y}\) are the standard deviations of the rank variables
    \end{itemize}

    Let us directly work with ranks to derive the formula. We denote \(X_i\) and 
    \(Y_i\) as the ranks. Then for each of these

    \begin{align*}
        \sum_{i=1}^{n} X_i &= 1 + 2 + \cdots + n \\
            &= \frac{n(n+1)}{2} \\
        \intertext{We note that the assumption of distinct ranks doesn't come into 
        play here. This is because in case of ties an average of the tied ranks is
        given to both the variables and the sum becomes the same in that case as well}
        \overline{X} &= \frac{1}{n} \sum_{i=1}^{n} X_i \\
            &= \frac{n + 1}{2} \\
        \text{Similarly, } \overline{Y} &= \frac{n + 1}{2}
    \end{align*}

    Now we calculate the variance.
    \begin{align*}
        \sigma_X^2 &= \frac{1}{n} \sum_{i=1}^{n} (X_i - \overline{X})^2 \\
            &= \frac{1}{n} \sum_{i=1}^{n} (X_i^2 - 2 X_i \overline{X} + \overline{X}^2) \\
            &= \frac{1}{n} \left[ \sum_{i=1}^{n} X_i^2 - 2 \overline{X} \sum_{i=1}^n X_i + n \overline{{X}^2} \right] \\
            &= \frac{1}{n} \left[ \sum_{i=1}^{n} X_i^2 - 2 \overline{X} (n \overline{X}) + n \overline{{X}^2} \right] \\
            &= \frac{1}{n} \left[ \sum_{i=1}^{n} X_i^2 - n \overline{X}^2 \right] \\
        \intertext{We use the assumption of distinct ranks here to get}
        \sum_{i=1}^{n} X_i^2 &= 1^2 + 2^2 + \cdots + n^2 = \frac{n(n+1)(2n+1)}{6}
        \intertext{Plugging in this value and the mean calculated earlier, we get}
        \sigma_X^2 &= \frac{1}{n} \left[ \frac{n(n+1)(2n+1)}{6} - n \left\{\frac{n+1}{2}\right\}^2 \right] \\
            &= \frac{1}{n} \left[ \frac{n(n+1)(2n+1)}{6} - \frac{n(n + 1)^2}{4} \right] \\
            &= \frac{1}{n} \frac{n(n+1)}{12} \left[ 2(2n + 1) - 3(n+1) \right] \\
            &= \frac{n + 1}{12} (n - 1) \\
            &= \frac{n^2 - 1}{12} \\
        \text{Similarly, we get } \sigma_Y^2 &= \frac{n^2 - 1}{12}
    \end{align*}

    Having calculated the variance of \(X_i, Y_i\), we calculate their covariance.
    \begin{align*}
        Cov(X, Y) &= \frac{1}{n} \sum_{i=1}^{n} (X_i - \overline{X}) (Y_i - \overline{Y}) \\
            &= \frac{1}{n} \sum_{i=1}^{n} [X_i Y_i - X_i \overline{Y} - \overline{X} Y_i + \overline{X} \overline{Y}] \\
            &= \frac{1}{n} \left\{ \sum_{i=1}^{n} X_i Y_i - \overline{Y} \sum_{i=1}^{n} X_i - \overline{X} \sum_{i=1}^{n} Y_i + n \overline{X} \overline{Y} \right\} \\
            &= \frac{1}{n} \left\{ \sum_{i=1}^{n} X_i Y_i - n \overline{Y} \overline{X} - n \overline{X} \overline{Y} + n \overline{X} \overline{Y} \right\} \\
            &= \frac{1}{n} \left\{ \sum_{i=1}^{n} X_i Y_i - n \overline{X} \overline{Y}\right\} \\
            &= \frac{1}{n} \sum_{i=1}^{n} X_i Y_i - \overline{X} \overline{Y} \\
            &= -\frac{1}{2n} \sum_{i=1}^{n} \left\{ X_i^2 + Y_i^2 -2 X_i Y_i - (X_i^2 + Y_i^2) \right\} - \overline{X} \overline{Y} \\
            &= \frac{1}{2n} \sum_{i=1}^{n} \left\{ X_i^2 + Y_i^2 \right\} -\frac{1}{2n} \sum_{i=1}^{n} \left\{ X_i^2 + Y_i^2 -2 X_i Y_i \right\} - \overline{X} \overline{Y} \\
            \intertext{We are once again invoking the assumption of distinct ranks to calculate the sum of \(X_i^2\) to get}
        Cov(X, Y) &= \frac{1}{2n} \times 2\cdot \frac{n(n+1)(2n+1)}{6} - \frac{1}{2n} \sum_{i=1}^{n} \left\{ X_i - Y_i \right\}^2 - \overline{X} \overline{Y} \\
            &= \frac{(n + 1)(2n + 1)}{6} - \frac{1}{2n} \sum_{i=1}^{n} d_i^2 - \left\{ \frac{n+1}{2} \right\}^2 \\
            &= \frac{n + 1}{12} \{2(2n + 1) - 3(n + 1)\} - \frac{1}{2n} \sum_{i=1}^{n} d_i^2 \\
            &= \frac{n + 1}{12} \cdot (n - 1) - \frac{1}{2n} \sum_{i=1}^{n} d_i^2 \\
            &= \frac{n^2 - 1}{12} - \frac{1}{2n} \sum_{i=1}^{n} d_i^2
    \end{align*}

    Where \(d_i = X_i - Y_i\) is the difference of ranks.

    Now we calculate the Spearman rank correlation coefficient using \eqref{eq:q3:spearman_general_formula}
    \begin{align*}
        r_s &= \frac{Cov(X, Y)}{\sigma_X \sigma_Y} \\
            &= \frac{\frac{n^2 - 1}{12} - \frac{1}{2n} \sum_{i=1}^{n} d_i^2}{ \frac{n^2 - 1}{12} } \\
        \therefore~ \Aboxed{r_s  &= 1 - \frac{6}{n(n^2 - 1)} \sum_{i=1}^{n} d_i^2}
            \numberthis \label{eq:q3:spearman_distinct_formula}
    \end{align*}

    Thus we have obtained the formula for calculating the Spearman rank correlation coefficient in case
    of distinct ranks.

    Now for the given data, we first convert the raw scores into ranks. These are tabulated below:

    \begin{center}
        \begin{tabular}{ | c | c | c | c | c | c |}
            \hline
            X & Y & rank \(X_i\) & rank \(Y_i\) & \(d_i\) & \(d_i^2\)\\
            \hline
            106 &	7  & 7  &   3  & 4 & 16\\
            100 &  	27 & 4  &   7  & -3 & 9\\
            86 & 2     & 1  &   1  & 0 & 0\\
            101 &	50 & 5  &   10 & -5 & 25\\
            99 &	29 & 2.5  & 8.5  & -6 & 36\\
            103 &	29 & 6  &  8.5 & -2.5 & 6.25\\
            99 &	20 & 2.5  & 6  & -3.5 & 12.25\\
            113 &	12 & 10  &  4  & 6 & 36\\
            112 &	6  & 9  &   2 & 7 & 49\\
            110 &	17 & 8  &   5 & 3 & 9\\
            \hline
        \end{tabular}
    \end{center}

    Note that due to ties, there are some fractional ranks. Suppose there was a tie
    between values at ranks 2 and 3. Then we assign both of these rank 2.5 which is
    the mean of 2 and 3.

    Now, we calculate the spearman rank correlation coefficient using the general 
    formula \eqref{eq:q3:spearman_general_formula}.
    \begin{align*}
        Var(\text{rank } X) &= 8.2 \\
        Var(\text{rank } Y) &= 8.2 \\
        Cov(\text{rank } X, \text{rank } Y) &= -1.725 \\
        \therefore r_s &= \frac{Cov(\text{rank } X, \text{rank } Y)}{\sqrt{Var(\text{rank } X) ~Var(\text{rank } Y)}} \\
            &= \frac{-1.725}{\sqrt{8.2 \times 8.2}} \\
            &= -0.210
    \end{align*}

    Now, using the Spearman rank correlation coefficient distinct rank formula
    \eqref{eq:q3:spearman_distinct_formula}.
    \begin{align*}
        r_s &= 1 - \frac{6}{n(n^2 - 1)} \sum_{i=1}^{n} d_i^2 \\
            &= -0.203
    \end{align*}

    We note that there is only an absolute difference of 0.007 in these calculated
    values which is about 3.5\% relative difference. We note that it's a small
    difference and the distinct rank formula may be useful even in case of ties
    for approximation purposes.
}

\item {
    Analysis of correlation and regression

    Consider the following data from the agricultural sector of Taiwan, 1958-1972.
    We are interested in seeing the effect of labor days (\(X_2\)) and the real capital input (\(X_3\))
    on the real gross product (\(Y\)).

    \begin{enumerate}
        \item Write the regression equation when regressing \(X_2\) and \(X_3\) on \(Y\)
        \item Derive the estimators for the coefficients using Ordinary Least Squares
        \item Find the estimates of \(Y\) using the estimators obtained. Also find the standard error.
        \item Find the \(R^2\) and adjusted-\(R^2\) values.
    \end{enumerate}

    \begin{tabularx}{\linewidth}{ |X|X|X|X| }
        \hline
        Year & Real gross product (millions of NT \$), \(Y\) & Labor days (millions of days), \(X_2\) & Real capital input (millions of NT \$), \(X_3\) \\
        \hline
        1958 & 16607.7 & 275.5 & 17803.7  \\
        1959 & 17511.3 & 274.4 & 18096.8  \\
        1960 & 20171.2 & 269.7 & 18271.8  \\
        1961 & 20932.9 & 267   & 19167.3  \\
        1962 & 20406   & 267.8 & 19647.6  \\
        1963 & 20831.6 & 275   & 20803.5  \\
        1964 & 24806.3 & 283   & 22076.6  \\
        1965 & 26465.8 & 300.7 & 23445.2  \\
        1966 & 27403   & 307.5 & 24939    \\
        1967 & 28628.7 & 303.7 & 26713.7  \\
        1968 & 29904.5 & 304.7 & 29957.8  \\
        1969 & 27508.2 & 298.6 & 31585.9  \\
        1970 & 29035.5 & 295.5 & 33474.5  \\
        1971 & 29281.5 & 299   & 34821.8  \\
        1972 & 31535.8 & 288.1 & 41794.3  \\
        \hline
    \end{tabularx}

    \textbf{Answer}

    The population regression equation is
    \[Y = \beta_1 + \beta_2 X_2 + \beta_3 X_3 + \epsilon\]
    where \(\epsilon\) is an error term.

    The sample regression equation using the estimators is
    \[Y_i = \widehat{\beta}_1 + \widehat{\beta}_2 X_{2i} + \widehat{\beta}_3 X_{3i} + \epsilon_i \]

    For a sample with \(n\) observations, we want to minimise
    \[S = \sum_{i=1}^{n} \left\{ Y_i - \widehat{\beta}_1 - \widehat{\beta}_2 X_{2i} - \widehat{\beta}_3 X_{3i} \right\}^2\]

    To do this, we use the first derivative tests and set the partial derivatives of \(S\)
    with respect to the estimators as 0. We get the equations
    \begin{align*}
        \pdv{S}{\widehat{\beta}_1} &= -2 \sum_{i=1}^{n} \left\{ Y_i - \widehat{\beta}_1 - \widehat{\beta}_2 X_{2i} - \widehat{\beta}_3 X_{3i} \right\} \\
        \pdv{S}{\widehat{\beta}_2} &= - \sum_{i=1}^{n} X_{2i} \left\{ Y_i - \widehat{\beta}_1 - \widehat{\beta}_2 X_{2i} - \widehat{\beta}_3 X_{3i} \right\} \\
        \pdv{S}{\widehat{\beta}_3} &= - \sum_{i=1}^{n} X_{3i} \left\{ Y_i - \widehat{\beta}_1 - \widehat{\beta}_2 X_{2i} - \widehat{\beta}_3 X_{3i} \right\} \\
    \end{align*}
    
    Setting these equations equal to zero, we obtain the normal equations
    \begin{align*}
        \sum_{i=1}^{n} \left\{ Y_i - \widehat{\beta}_1 - \widehat{\beta}_2 X_{2i} - \widehat{\beta}_3 X_{3i} \right\} &= 0 \\
        \implies \sum_{i=1}^{n} Y_i - n \widehat{\beta}_1 - \widehat{\beta}_2 \sum_{i=1}^{n} X_{2i} - \widehat{\beta}_3 \sum_{i=1}^{n} X_{3i} &= 0 \\
        \sum_{i=1}^{n} X_{2i} \left\{ Y_i - \widehat{\beta}_1 - \widehat{\beta}_2 X_{2i} - \widehat{\beta}_3 X_{3i} \right\} &= 0  \\
        \implies \sum_{i=1}^{n} X_{2i} Y_i - \widehat{\beta}_1 \sum_{i=1}^{n} X_{2i} - \widehat{\beta}_2 \sum_{i=1}^{n} X_{2i}^2 - \widehat{\beta}_3 \sum_{i=1}^{n} X_{2i} X_{3i} &= 0 \\
        \sum_{i=1}^{n} X_{3i} \left\{ Y_i - \widehat{\beta}_1 - \widehat{\beta}_2 X_{2i} - \widehat{\beta}_3 X_{3i} \right\} &= 0 \\
        \implies \sum_{i=1}^{n} X_{3i} Y_i - \widehat{\beta}_1 \sum_{i=1}^{n} X_{3i} - \widehat{\beta}_2 \sum_{i=1}^{n} X_{2i} X_{3i} - \widehat{\beta}_3 \sum_{i=1}^{n} X_{3i}^2 &= 0
    \end{align*}

    Putting the values from the data, we obtain the equations as:
    \begin{align*}
        371030 - 15 \widehat{\beta}_1 - 4310.2 \widehat{\beta}_2 - 382599.5 \widehat{\beta}_3 &= 0 \\
        107449076 - 4310.2 \widehat{\beta}_1 - 1241590.88 \widehat{\beta}_2 - 110882249 \widehat{\beta}_3 &= 0 \\
        9907328434 - 382599.5 \widehat{\beta}_1 - 110882249 \widehat{\beta}_2 - 10512155203.39 \widehat{\beta}_3 &= 0
    \end{align*}

    Solving this system of equations, we get the values as
    \[\widehat{\beta}_1 = -28067.2, \widehat{\beta}_2 = 147.94, \widehat{\beta}_3 = 0.4036\]

    Using these, we calculate the estimated \(Y\) values, which are written in the table below:
    \begin{center}
    \begin{tabularx}{0.8\linewidth}{|X|X|X|X|X|}
        \hline
        Year & \(Y\)   & \(X_2\) & \(X_3\) & Estimated \(Y\) \\
        \hline
        1958 & 16607.7 & 275.5 & 17803.7 & 19875.84332 \\
        1959 & 17511.3 & 274.4 & 18096.8 & 19831.40448 \\
        1960 & 20171.2 & 269.7 & 18271.8 & 19206.71648 \\
        1961 & 20932.9 & 267   & 19167.3 & 19168.70228 \\
        1962 & 20406   & 267.8 & 19647.6 & 19480.90336 \\
        1963 & 20831.6 & 275   & 20803.5 & 21012.5926  \\
        1964 & 24806.3 & 283   & 22076.6 & 22709.93576 \\
        1965 & 26465.8 & 300.7 & 23445.2 & 25880.84072 \\
        1966 & 27403   & 307.5 & 24939   & 27489.7304  \\
        1967 & 28628.7 & 303.7 & 26713.7 & 27643.82732 \\
        1968 & 29904.5 & 304.7 & 29957.8 & 29101.08608 \\
        1969 & 27508.2 & 298.6 & 31585.9 & 28855.75324 \\
        1970 & 29035.5 & 295.5 & 33474.5 & 29159.3782  \\
        1971 & 29281.5 & 299   & 34821.8 & 30220.93848 \\
        1972 & 31535.8 & 288.1 & 41794.3 & 31422.49348 \\
        \hline
    \end{tabularx}
    \end{center}

    The standard error is given by
    \[\sqrt{\frac{\sum(Y - Y_{est})^2}{n}} = 1416.13 \text{ NT \$}\]

    Now we calculate the \(R^2\) and adjusted-\(R^2\) values.
    \begin{align*}
        R^2 &= 1 - \frac{\text{Residual Sum of Squares}}{\text{Total Sum of Squares}} \\
            &= 1 - \frac{\sum \left( Y_i - \widehat{Y}_i \right)^2}{\sum \left( Y_i - \overline{Y} \right)^2} \\
            &= 0.91 \\
        \text{Adjusted-\(R^2\), } \overline{R}^2 &= 1 - \frac{\frac{\text{Residual Sum of Squares}}{n - k}}{\frac{\text{Total Sum of Squares}}{n - 1}} \\
        \intertext{Here \(n\) is the number of variables in the sample (=15) and \(k\) is the number of parameters in the regression (=3)}
        \implies  \overline{R}^2 &= 1 - \frac{\sum \left( Y_i - \widehat{Y}_i \right)^2 / (n - k)}{\sum \left( Y_i - \overline{Y} \right)^2 / (n - 1)} \\
            &= 0.89
    \end{align*}

    Thus we see that 90\% of the variation in \(Y\) is explained by the variations 
    in \(X_2\) and \(X_3\) through this model.
}


\item {
    Time Series Analysis

    Show that every stationary MA(2) process without a unit root can be converted to an invertible
    process by suitably changing the coefficients and white noise random variables.
    Note that this can actually be done for all MA(q) processes. Is the following
    MA(2) process invertible?
    \[X_t = e_t - 3.2 e_{t-1} + 0.6 e_{t-2}\]
    Here \(e_t\)'s are white noise random variables normally distributed with mean
    0 and variance 2. If not, convert it to an invertible MA(2) process.

    \textbf{Answer}

    A general MA(2) process looks like the following:
    \begin{align*}
        X_t &= e_t + \lambda_1 e_{t-1} + \lambda_2 e_{t-2} \\
        \intertext{We can use the backshift operator \(B\) to write}
        X_t &= e_t + \lambda_1 B(e_{t}) + \lambda_2 B^2(e_{t}) \\
        \implies X_t &= (1 + \lambda_1 B + \lambda_2 B^2) e_{t} \\
        \intertext{We factorise the polynomial into linear factors to get}
        X_t &= (1 - \mu_1 B)(1 - \mu_2 B) e_t
    \end{align*}

    Thus we get the general characteristic polynomial of the MA(2) process as
    \((1 - \mu_1 B)(1 - \mu_2 B)\). The zeros of the polynomial are \(\frac{1}{\mu_1}\)
    and \(\frac{1}{\mu_2}\) respectively.

    We note that the way to find if the process is invertible is that the absolute
    value of the zeros of the characteristic polynomial are greater than 1. If not, there would be atleast one zero which is less than 1. We note that the
    absence of unit roots means that no zero is exactly equal to 1.

    We show that a process with the characteristic polynomial with one root \(\mu\) taken
    as the reciprocal and variance of the white noise multiplied by \(\mu^2\) yields
    a process with the same autocovariance function.

    Let us define \(\lambda_0 = 1\). Then,
    \begin{align*}
        Cov(X_t, X_{t+\tau}) &= E(X_t X_{t+\tau}) - E(X_t)E(X_{t+\tau}) \\
            &= E(X_t X_{t+\tau}) \tag*{(since \(E(X_t) = 0 \,\forall t\))} \\
            &= E\left[\left\{\sum_{j=0}^{q}\lambda_j e_{t-j}\right\}\left\{\sum_{k=0}^{q}\lambda_k e_{t-k+\tau}\right\}\right] \\
            &= \sum_{j=0}^{q} \sum_{k=0}^{q} \lambda_j \lambda_k E(e_{t-j} e_{t-k+\tau}) \\
        \intertext{Since \(e_t\)'s are independent and identically distributed with variance as
        \(\sigma^2\)}
        Cov(X_t, X_{t+\tau}) &= \sum_{j=0}^{q} \sum_{k=0}^{q} \lambda_j \lambda_k \sigma^2 \delta_{t-j,t-k+\tau}
            \tag*{(Where \(\delta_{ij}\) is the Kronecker delta)}
    \end{align*}

    To remove the Kronecker delta, We make the following observations:
    \begin{itemize}
        \item When \(\tau = 0\), the \((j, k)\) pairs which will be included are \((0, 0), (1, 1), \ldots, (q, q)\)
        \item For \(\tau = 1\), we'll have \((0, 1), (1, 2), \ldots, (q-1, q)\)
        \item For \(\tau = 2\), we'll have \((0, 2), (1, 3), \ldots, (q-2, q)\)
        \item \(\vdots\)
        \item For \(\tau = q-1\), we'll have \((0, q-1), (1, q)\)
        \item For \(\tau = q\), we'll have \((0, q)\)
        \item For \(\tau > q\), \(t - j \neq t - k + \tau \,\forall j, k = 0, \ldots, q\)
    \end{itemize}

    So we'll have the following simplification:
    \begin{equation}
        Cov(X_t, X_{t+\tau}) = \begin{cases}
            \sigma^2 \sum_{j=0}^{q-\tau} \lambda_j \lambda_{j+\tau} & \text{if } \tau = 0, 1, \ldots, q \\
            0 & \tau > q
        \end{cases}
    \end{equation}

    Now for a process \(X_t = (1 - \mu_1 B)(1 - \mu_2 B)e_t\), we have \(\lambda_1 = -\mu_1 - \mu_2\)
    and \(\lambda_2 = \mu_1 \mu_2\), then we get
    \begin{align*}
        Cov(X_t, X_t) &= \sigma^2 [1 + (\mu_1 + \mu_2)^2 + \mu_1^2 \mu_2^2] \\
        Cov(X_t, X_{t+1}) &= \sigma^2 [(-\mu_1-\mu_2) + (-\mu_1-\mu_2)(\mu_1 \mu_2)] \\
            &= - \sigma^2 (\mu_1 + \mu_2) (1 + \mu_1 \mu_2) \\
        Cov(X_t, X_{t+2}) &= \sigma^2 \mu_1 \mu_2 \\
        Cov(X_t, X_{t+\tau}) &= 0 \,\forall \tau > 2
    \end{align*}

    If we replace \(\mu_1\) by \(\frac{1}{\mu_1}\) in the above equations, we'll get
    \begin{align*}
        Cov(X_t, X_t) &= \sigma^2 \left[1 + \left(\frac{1}{\mu_1} + \mu_2\right)^2 + \frac{1}{\mu_1^2} \mu_2^2\right] \\
            &= \frac{\sigma^2}{\mu_1^2} \left[\mu_1^2 + \left(1 + \mu_1 \mu_2\right)^2 + \mu_2^2\right] \\
            &= \frac{\sigma^2}{\mu_1^2} \left[\mu_1^2 + 1 + 2 \mu_1 \mu_2 + \mu_1^2 \mu_2^2 + \mu_2^2\right] \\
            &= \frac{\sigma^2}{\mu_1^2} \left[1 + (\mu_1 + \mu_2)^2 + \mu_1^2 \mu_2^2\right] \\
        Cov(X_t, X_{t+1}) &= - \sigma^2 \left(\frac{1}{\mu_1} + \mu_2\right) \left(1 + \frac{1}{\mu_1} \mu_2\right) \\
            &= - \frac{\sigma^2}{\mu_1^2} \left(1 + \mu_1 \mu_2\right) \left(\mu_1 + \mu_2\right) \\
        Cov(X_t, X_{t+2}) &= \sigma^2 \frac{1}{\mu_1} \mu_2 \\
        &= \frac{\sigma^2}{\mu_1^2} ~\mu_1 \mu_2 \\
        Cov(X_t, X_{t+\tau}) &= 0 \,\forall \tau > 2
    \end{align*}

    We observe that we have the exact same autocovariance equations with the variance
    \(\sigma^2\) divided by \(\mu^2\).
    
    Thus, we can change a root of the characteristic polynomial to its reciprocal by suitably
    changing the variance of the white noise variables to get the same autocovariance function.
    This way, we can change all roots which are less than 1 to become more than 1 and obtain an
    invertible process this way.

    For the process \(X_t = e_t - 3.2 e_{t-1} + 0.6 e_{t-2}\), the characteristic polynomial
    is \(\theta(B) = 1 - 3.2 B + 0.6 B^2 = (1 - 3 B)(1 - 0.2 B)\). The zeros of this
    polynomial are \(\frac{1}{3}, 5\). Since one of them is less than 1, this process is not
    invertible.

    We can make this invertible by the procedure outlined above. We'll replace the root
    \(\frac{1}{3}\) by 3 and change the variance of white noise which is currently 2 to
    \(\frac{2}{\frac{1}{3}^2} = 18\). Thus, we obtain the process
    \begin{align*}
        X_t &= (1 - \frac{1}{3} B)(1 - 0.2 B) e_t \\
            &= e_t - \frac{8}{15} e_{t-1} + \frac{1}{15} e_{t-2}
    \end{align*}
    where \(e_t\)'s are white noise variables normally distributed with mean 0 and variance 18.
}


\item {
    Time Series Analysis

    Consider the stationary ARMA(p, q) process. Elaborate the general method of finding the
    variance and covariances. Use the method to find the variance and covariances \&
    correlations of time difference upto 3 (i.e. \(\rho_1, \rho_2, \rho_3\)) of the stationary ARMA(1, 1)
    process.

    \textbf{Answer}

    Let \(\left\{X_t\right\}_{t \in \mathbb{N}}\) be a time series following the
    stationary ARMA(p, q) process. Then we have the following recurrence relation:

    \[X_t = \sum_{j=1}^{p}\alpha_j X_{t-j} + \sum_{j=1}^{q} \beta_j e_{t-j} + e_t\]

    where \(e_t\)'s are independent and identically distributed random variables following
    a normal distribution with mean 0 and variance 1. We also take the boundary condition as
    \(X_t = 0 ~\forall t < max(p, q)\)

    Since this is a stationary process, it is implied that it will also satisfy
    wide sense stationarity, which means that
    \begin{align*}
        E(X_t) &= \mu ~\forall t \\
        Cov(X_t, X_s) &= f(t - s)
    \end{align*}

    i.e. the mean of all the random variables of the process is constant and
    the covariance of the random variables at two time instances of the process
    depends only on the time difference between them.

    We first find the mean of the random variables of the process:
    \begin{align*}
        X_t &= \sum_{j=1}^{p}\alpha_j X_{t-j} + \sum_{j=1}^{q} \beta_j e_{t-j} + e_t \\
        \implies E(X_t) &= E\left(\sum_{j=1}^{p}\alpha_j X_{t-j} + \sum_{j=1}^{q} \beta_j e_{t-j} + e_t\right) \\
        \implies E(X_t) &= \sum_{j=1}^{p}\alpha_j E(X_{t-j}) + \sum_{j=1}^{q} \beta_j E(e_{t-j}) + E(e_t)
        \intertext{Now we use the fact that \(e_t\)'s have a mean of 0 to get}
        E(X_t) &= \sum_{j=1}^{p}\alpha_j E(X_{t-j}) \\
        \implies \mu &= \sum_{j=1}^{p}\alpha_j \mu \tag*{(Since the process is stationary)} \\
        \implies \left(\sum_{j=1}^{p}\alpha_j - 1\right) \mu &= 0 \\
        \intertext{Now assuming that \(\sum_{j=1}^{p}\alpha_j \neq 0\), we get}
        \mu &= 0
    \end{align*}

    Therefore we have the following result:
    \begin{equation}
        E(X_t) = 0 ~\forall t \label{eq:q6:meanzero}
    \end{equation}

    We denote \(\gamma_\tau\) as the covariance of random variables in this process
    at time \(\tau\) apart. Then,
    \begin{align*}
        X_t &= \sum_{j=1}^{p}\alpha_j X_{t-j} + \sum_{j=1}^{q} \beta_j e_{t-j} + e_t \\
        \intertext{Multiplying by \(X_{t-\tau}\) on both sides, we get}
        X_t X_{t-\tau} &= \sum_{j=1}^{p}\alpha_j X_{t-j} X_{t-\tau} + \sum_{j=1}^{q} \beta_j e_{t-j} X_{t-\tau} + e_t X_{t-\tau} \\
        \implies E\left(X_t X_{t-\tau}\right) &= E\left(\sum_{j=1}^{p}\alpha_j X_{t-j} X_{t-\tau} + \sum_{j=1}^{q} \beta_j e_{t-j} X_{t-\tau} + e_t X_{t-\tau}\right) \\
        \implies E\left(X_t X_{t-\tau}\right) &= \sum_{j=1}^{p}\alpha_j E\left(X_{t-j} X_{t-\tau}\right) + \sum_{j=1}^{q} \beta_j E\left(e_{t-j} X_{t-\tau}\right) + E\left(e_t X_{t-\tau}\right) \tag*{(Since expectation is a linear operator)} \\
        \intertext{Using \eqref{eq:q6:meanzero}, we have \(Cov(X_t, X_s) = E(X_t X_s) - E(X_t)E(X_s) = E(X_t X_s)\)}
        \implies Cov\left(X_t, X_{t-\tau}\right) &= \sum_{j=1}^{p}\alpha_j Cov\left(X_{t-j}, X_{t-\tau}\right) + \sum_{j=1}^{q} \beta_j E\left(e_{t-j} X_{t-\tau}\right) + E\left(e_t X_{t-\tau}\right) \\
        \implies \gamma_{\tau} &= \sum_{j=1}^{p}\alpha_j \gamma_{\tau - j} + \sum_{j=1}^{q} \beta_j E\left(e_{t-j} X_{t-\tau}\right) + E\left(e_t X_{t-\tau}\right)
    \end{align*}

    Now we note that \(X_{t-\tau}\) is a function of the white noise variables \(e_1,\ldots,e_{t-\tau}\) and since
    they are independent from each other, \(e_t\) is independent from all of \(e_1,\ldots,e_{t-\tau}\) and thus \(E(e_t X_{t-\tau}) = Cov(e_t, X_{t-\tau}) + \cancelto{0}{E(e_t)} E(X_{t-\tau}) = Cov(e_t, X_{t-\tau}) = 0\).
    Therefore, we get
    \begin{equation}
        \gamma_{\tau} = \sum_{j=1}^{p}\alpha_j \gamma_{\tau - j} + \sum_{j=1}^{q} \beta_j E\left(e_{t-j} X_{t-\tau}\right) \label{eq:q6:arma_var}
    \end{equation}

    Using the previous logic, we can see that \(E(e_t X_s) = 0\) whenever \(t > s\) i.e. \(e_t\) and \(X_s\) are
    independent whenever \(t > s\). In lieu of this observation, we split into the following two cases:

    \renewcommand{\labelitemi}{\textendash}
    \begin{itemize}
        \item \textbf{Case 1: } \(\tau > q\)
        
        Here, \(j < \tau \implies t - j > t - \tau\) for all \(j = 1, \ldots, q\). This implies, by our previous observation,
        that \(E(e_{t-j} X_{t-\tau}) = 0\) for all \(j = 1,\ldots,q\)

        Thus the equation simplifies to:
        \begin{equation}
            \gamma_{\tau} = \sum_{j=1}^{p}\alpha_j \gamma_{\tau - j} \label{eq:q6:case1}
        \end{equation}

        \item \textbf{Case 2: } \(0 < \tau \leq q\)
        
        In this case, we split up the summation into two parts as:
        \begin{align*}
            \sum_{j=1}^{q} \beta_j E(e_{t-j} X_{t-\tau}) &= \cancelto{0}{\sum_{j=1}^{\tau - 1} \beta_j E(e_{t-j} X_{t-\tau})} + \sum_{j=\tau}^{q} \beta_j E(e_{t-j} X_{t-\tau}) \\
                &= \sum_{j=\tau}^{q} \beta_j E(e_{t-j} X_{t-\tau})
        \end{align*}

        For each of the terms we'll need to expand \(X_{t-\tau}\) to find the coefficient
        of \(e_{t-j}\) in it, which will give us the expectation.

        Thus we get the equation as:
        \begin{equation}
            \gamma_{\tau} = \sum_{j=1}^{p}\alpha_j \gamma_{\tau - j} + \sum_{j=\tau}^{q} \beta_j E(e_{t-j} X_{t-\tau})
                \label{eq:q6:case2}
        \end{equation}
    \end{itemize}

    Now, we find the variance of \(X_t\).
    \begin{align*}
        X_t &= \sum_{j=1}^{p}\alpha_j X_{t-j} + \sum_{j=1}^{q} \beta_j e_{t-j} + e_t \\
        \intertext{Multiplying by \(X_t\) on both sides, we get}
        X_t^2 &= \sum_{j=1}^{p}\alpha_j X_{t-j} X_t + \sum_{j=1}^{q} \beta_j e_{t-j} X_t + e_t X_t \\
        \implies E(X_t^2) &= \sum_{j=1}^{p}\alpha_j E(X_{t-j} X_t) + \sum_{j=1}^{q} \beta_j E(e_{t-j} X_t) + E(e_t X_t) \\
        \implies Var(X_t) &= \sum_{j=1}^{p}\alpha_j \gamma_j + \sum_{j=1}^{q} \beta_j E(e_{t-j} X_t) + E\left(e_t \left\{\sum_{j=1}^{p}\alpha_j X_{t-j} + \sum_{j=1}^{q} \beta_j e_{t-j} + e_t\right\}\right) \\
        \implies Var(X_t) &= \sum_{j=1}^{p}\alpha_j \gamma_j + \sum_{j=1}^{q} \beta_j E(e_{t-j} X_t) + \sum_{j=1}^{p}\alpha_j \cancelto{0}{E(e_t X_{t-j})} + \sum_{j=1}^{q} \beta_j \cancelto{0}{E(e_t e_{t-j})} + E(e_t^2)
    \end{align*}
    
    Therefore, we have the variance equation as:
    \begin{equation}
        Var(X_t) = \sum_{j=1}^{p}\alpha_j \gamma_j + \sum_{j=1}^{q} \beta_j E(e_{t-j} X_t) + 1 \label{eq:q6:var}
    \end{equation}

    Now we consider a stationary ARMA(1, 1) process with the equation
    \[X_t = \alpha X_{t-1} + \beta e_{t-1} + e_t\]

    To find the variance, we use \eqref{eq:q6:var}.
    \begin{align*}
        Var(X_t) &= \sum_{j=1}^{p}\alpha_j \gamma_j + \sum_{j=1}^{q} \beta_j E(e_{t-j} X_t) + 1 \\
            &= \alpha \gamma_1 + \beta E(e_{t-1} X_t) + 1 \\
            &= \alpha \gamma_1 + \beta E\left(e_{t-1} \left\{\alpha X_{t-1} + \beta e_{t-1} + e_t\right\} \right) + 1 \\
            &= \alpha \gamma_1 + \alpha \beta E(e_{t-1} X_{t-1}) + \beta^2 E(e_{t-1}^2) + \beta \cancelto{0}{E(e_{t-1} e_t)} + 1
    \end{align*}

    Now using \(E(e_{t-1}^2 = Var(e_{t-1}) = 1)\) and \(E(e_t X_t) = 1 ~\forall t\), we get
    \begin{equation}
        Var(X_t) = \alpha \gamma_1 + \alpha \beta + \beta^2 + 1 \label{eq:q6:var_calc}
    \end{equation}

    Note we have yet to find \(\gamma_1\) which we'll do next. Note that since \(0 < \tau = 1 \leq q = 1\),
    we'll use \eqref{eq:q6:case2} to calculate.
    \begin{align*}
        \gamma_{\tau} &= \sum_{j=1}^{p}\alpha_j \gamma_{\tau - j} + \sum_{j=\tau}^{q} \beta_j E(e_{t-j} X_{t-\tau}) \\
        \implies \gamma_1 &= \alpha \gamma_0 + \beta E(e_{t-1} X_{t-1}) \\
        \implies \gamma_1 &= \alpha \gamma_0 + \beta \\
        \implies \gamma_1 &= \alpha Var(X_t) + \beta \numberthis \label{eq:q6:gamma1_var} \\
        \intertext{Now using \eqref{eq:q6:var_calc}, we get}
        \gamma_1 &= \alpha (\alpha \gamma_1 + \alpha \beta + \beta^2 + 1) + \beta \\
        \implies \gamma_1 &= \alpha^2 \gamma_1 + \alpha^2 \beta + \alpha \beta^2 + \alpha + \beta \\
        \implies (1 - \alpha^2) \gamma_1 &= \alpha^2 \beta + \alpha \beta^2 + \alpha + \beta \\
        \implies \gamma_1 &= \frac{\alpha^2 \beta + \alpha \beta^2 + \alpha + \beta}{1 - \alpha^2} 
            \numberthis \label{eq:q6:gamma1_ans}
    \end{align*}

    Now that we have found \(\gamma_1\), we'll describe the other results using it since
    they get very messy otherwise. From \eqref{eq:q6:gamma1_var}, we have
    \begin{align*}
        \gamma_1 &= \alpha Var(X_t) + \beta \\
        \implies \gamma_1 &= \alpha \gamma_0 + \beta \\
        \intertext{Now dividing both sides by \(\gamma_0\) and setting \(\frac{\gamma_1}{\gamma_0}\) to \(\rho_1\), we get}
        \rho_1 &= \alpha + \frac{\beta}{\gamma_0} \\
        \implies \rho_1 &= \alpha + \frac{\beta}{ \alpha \gamma_1 + \alpha \beta + \beta^2 + 1} \tag*{(Using \eqref{eq:q6:var_calc})}
    \end{align*}

    For \(\tau > 1 = q\), we can use the equation for case 1 i.e. \eqref{eq:q6:case1}. We have
    \begin{align*}
        \gamma_{\tau} &= \sum_{j=1}^{p}\alpha_j \gamma_{\tau - j} \\
        \implies \gamma_\tau &= \alpha \gamma_{\tau-1} \\
        \intertext{Now dividing both sides by the variance to the correlations,}
        \rho_\tau &= \alpha \rho_{\tau-1} \\
        \therefore~ \rho_2 &= \alpha \rho_1 = \alpha^2 + \frac{\alpha \beta}{ \alpha \gamma_1 + \alpha \beta + \beta^2 + 1} \\
        \rho_3 &= \alpha \rho_2 = \alpha^3 + \frac{\alpha^2 \beta}{ \alpha \gamma_1 + \alpha \beta + \beta^2 + 1}
    \end{align*}

    Therefore, we have found the values of \(\rho_1, \rho_2, \rho_3\) as
    \begin{align*}
        \rho_1 &= \alpha + \frac{\beta}{ \alpha \gamma_1 + \alpha \beta + \beta^2 + 1} \\
        \rho_2 &= \alpha^2 + \frac{\alpha \beta}{ \alpha \gamma_1 + \alpha \beta + \beta^2 + 1} \\
        \rho_3 &= \alpha^3 + \frac{\alpha^2 \beta}{ \alpha \gamma_1 + \alpha \beta + \beta^2 + 1}
        \intertext{where, }
        \gamma_1 &= \frac{\alpha^2 \beta + \alpha \beta^2 + \alpha + \beta}{1 - \alpha^2}
    \end{align*}
}	


\end{enumerate}

\end{document}
